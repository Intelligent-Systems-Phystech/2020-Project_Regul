\begin{comment}

\begin{thebibliography}{4}
\bibitem{bib_1} Wang J. et al. Learning credible models //Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. – ACM, 2018. – С. 2417-2426.
\bibitem{bib_2} Tibshirani R. Regression shrinkage and selection via the lasso //Journal of the Royal Statistical Society: Series B (Methodological). – 1996. – Т. 58. – №. 1. – С. 267-288.
\bibitem{bib_3} Zou H., Hastie T. Regularization and variable selection via the elastic net //Journal of the royal statistical society: series B (statistical methodology). – 2005. – Т. 67. – №. 2. – С. 301-320.
\bibitem{bib_4} Ссылка на нашу статью.
\end{thebibliography}
\end{comment}





\begin{comment}


\section{Функция ошибки и критерии качества модели}

Ключевой идеей данной работы является построение новой функции ошибки. 

Предлагается использовать композитную функцию ошибки. Она состоит из нескольких слагаемых. Первое слагаемое соответствует точности восстановления зависимой переменной. Второе слагаемое это точность реконструкции независимой переменной автокодировщиком. Задача \eqref{eq102} является задачей минимизации функции $S$, включающее слагаемое \eqref{eq3} и \eqref{eq54} для оптимизации параметров модели \eqref{eq58}

\begin{equation}
S(\wm|\mathbf{\gamma},D) = S(\wm),
\end{equation}
\begin{equation}\label{eq11}
S = \lambda_1S_1+\dots+\lambda_kS_k = \mathbb{\lambda}^{\mathsf{T}}\mathbb{S}.
\end{equation}
Требуется создать каталог слагаемых функции ошибки.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|}
\hline
  Тип регуляризатора   & Роль  \\
  \hline
  $||\mathbf{y} - f(\w)||^2_2 = E_y$,   & Ошибка выхода нейронной сети \\
  \hline
  $||\xb-\mathbf{r}(\xb)||^2_2$ & Ошибка восстановления на каждом слое \\
  \hline
  $||\wm-\wm_0||_1$, $||\wm-\wm_0||^2_2$, & \\
  \hline
  $||\w-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от тождественного преобразования \\
  \hline
  $||\w\w^T-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от метода главных компонент \\
  \hline
\end{tabular}
\caption{Каталог слагаемых функции ошибки}
\end{center}
\end{table}

%\begin{enumerate}
%\item[1)]
%$||\mathbf{y} - f(\w)||^2_2 = E_y$,
%\item[2)]
%$||\xb-\mathbf{r}(\xb)||^2_2$,
%\item[3)]
%$||\wm-\wm_0||_1$,
%$||\wm-\wm_0||^2_2$,
%\item[4)]
%$||\w-\mathbf{I}||$,
%\item[5)]
%$||\w\w^T-\mathbf{I}||$,
%\end{enumerate}

Ошибка $E_k$ для каждого слоя будет разная. Пусть $\w$ - матрица параметров одного слоя нейросети, тогда одной строке из этой матрицы будет соответствовать один нейрон. Каждый слой нейросети рассматривается либо в качестве скрытого слоя нейросети, либо в качестве слоя автоенкодера. Тогда $E_x$ – ошибка восстановления на каждом слое. Для каждого слоя с номером $k$ задается ошибка этого слоя
\begin{equation}\label{eq4}
E_k = \lambda_xE_x(\xb-\mathbf{r})+\lambda_yE_y(y-f(\w)) +\lambda_wE(\w)_\text{Frobenius}. 
\end{equation}
Норма фробениуса представляет попарное расстояние от нейрона до нейрона. Требуется увеличить ее значение, чтобы нейроны на одном слое нейросети существенно отличались. Задается одна функция ошибки, с помощью которой будет оптимизироваться структура модели.
Функция ошибки включает штрафы на нейрон, штраф на то, что матрица одного слоя отличается от чистого метода главных компонент. Также требуется исследовать условия перехода от метода главных компонент к методу независимых компонент.

Функция ошибки содержит произведение $k$ элементов на количество ошибок, описанных в таблице, другими словами, на введенное число штрафов. Каждый слой нейросети содержит ошибку и маску $\Gamma$.
Рассморим один слой. Его качественная оценка состоит из слагаемых $||\w_2||, ||\w_1||, E_x, E_y$. А также $||\w||_{\text{Frobenius}}$, которая указывает на разнообразие нейронов в слое. Получается 5*4 слагаемых для 4 слоев нейросети. 
Часть слагаемых может быть вычислена и оптимизирована без основного цикла оптимизации. Автоенкодер требует оптимизации только при наличии 2 слоев и более, потому что если только один – то это метод главных компонент. 

В прошлой работе маска $\Gamma$ оптимизировалась генетическим алгоритмом. В текущей работе необхожимо сделать $\Gamma$ функцией от гиперпараметров, а конкретно от результатов анализа функции ошибки.
При оптимизации используется метод бустрепа и вместо каждого $\w$ оптимального имеется набор оптимальных $\w$, с помощью которых вычисляется ковариационная матрица $A$. Так как дисперсии складываются -  оцениваются var нейрона и var слоя. В прошлой работе в оптимизации не использавалась дисперсия функции ошибки.
Предлагается декомпозировать дисперсию и использовать ее для выбора структуры $\Gamma$. 


???Нейроны в слоях нейросети взаимозаменяемы и
от мультистарта до мультистарта они меняются местами. Желательно решить эту проблему в данной работе. 

Требуется создать расписание оптимизации параметров регуляризации $\lambda$. Требуется назначать метапараметр в зависимости от номера итерации. Если оптимизация сети только началась, то нам важно подготовить выборку, чтобы на последнем слое нейросети она была простой. Поэтому требуется, чтобы начальные автоенкодеры восстанавливали исходный объект хорошо. Для выбора $\lambda$ предполагается использовать эмпирический подход и исследовать различные техники, как например подходы для выбора шага обучения. Лямбда – точка парето оптимального фронта.

В прошлой работе поиск структуры работал через добавление или удаление элементов из структуры. Это решение по сути является лихорадочным метанием, которое работает как полный перебор. Требуется предложить стратегию направленного поиска. Такая стратегия, конечно, работает чуть хуже чем полный перебор. 
Алгоритм градиентного спуска сходится довольно быстро. Требуется исследовать возможность поиска оптимальной структуры через сложность. 




В работе \cite{bib_1} представлена таблица свойств различных регуляризаций. В работе рассматриваются нормы со степенью один и два, то есть метрики $L1$ и $L2$. Требуется исследовать свойства метрика с различными значениями степени. 
Первое слагаемое $E_{\textbf{x}}$~--- это функция ошибки реконструкции объекта стеком автокодировщиков. Второе слагаемое $S$~--- это функция ошибки нейросети.

При выборе моделей используется три вида критериев качества: точность, устойчивость и сложность.

\paragraph{Точность.}

Когда в качестве используемой модели выступает нейросеть или линейная регрессия, 
то функция ошибки имеет вид:
\begin{equation}\label{eq3}
S = \sum_{i\in\mathcal{I}}\big(y_i-f(\xb_i)\big)^2.
\end{equation}
Эта функция ошибки включает в себя полученные предсказания модели и значения зависимых переменных. В задачах регрессии точность аппроксимации имеет вид: 
\begin{equation}\label{eq106}
\text{MAE} =\frac{ \sum\limits_{i=1}^m|y_i-f(\xb_i)|}{m}.
\end{equation}

При включении в модель \eqref{eq44} метода главных компонент или автокодировщика, метки объектов не используются. Функция ошибки штрафует невязки восстановленного объекта:
\begin{equation}\label{eq4}
E_\mathbf{x} = \sum_{i\in\mathcal{I}}\left\Vert{\xb_i-\mathbf{r}(\xb_i)}\right\Vert_2^2,
\end{equation}
где $\mathbf{r}(\x)$  это линейная реконструкция объекта $\x$. Функция \eqref{eq4} с аддитивной регуляризацией:
\begin{equation}\label{eq54}
E_\mathbf{x} = \frac{1}{2m}\sum\limits_{i=1}^m||\mathbf{r}(\textbf{x}_i,\textbf{W}_{\text{AE}}) - \x_i||^2 + \lambda^2||\w||_\text{Frobenius}^2,
\end{equation}
где $m$~---~ число элементов в обучающей выборке.
Параметры автокодировщика
\begin{equation}
\textbf{W}_{\text{AE}} = \{\w^{'},\w,\mathbf{b}^{'},\mathbf{b}\}
\end{equation}
оптимизированы таким образом (\ref{eq4}), чтобы приблизить реконструкцию $\mathbf{r}(\x)$ к исходному вектору $\x$.

Процедура оптимизации параметров композитной функции \eqref{eq58}:

\begin{enumerate}
\item[1)]
оптимизируются параметры модели согласно \eqref{eq54},
\item[2)]
заданные параметры фиксируются,
\item[3)]
оптимизируются параметры согласно \eqref{eq3}.
\end{enumerate}

\paragraph{Сложность.}
Введем отношение порядка $\succ$ на множестве значений сложности. Это отношение задается множеством параметров модели:
\begin{enumerate}
\item[1)] один параметр: $w\in \mathbb{R}^1 \succ w \in \lambda_1 [0,1] +\lambda_0 \succ w\in c +\lambda_0$,
\item[2)] вектор(нейрон): $\wm\in \mathbb{R}^n \succ \left\Vert \wm \right\Vert^2 =1 \succ \wm= \text{const}$,
\item[3)] матрица(слой): $\wvec\in \mathbb{R}^{c{\times}n} \succ \wvec^\mathsf{T}\wvec = \mathbf{I} \succ \wvec= \text{const}$.
\end{enumerate}

Множество, которому принадлежит сложность модели – порядковое. Исходя из введенного понятия сложности модели упорядочены 
по возрастанию сложности:
\begin{enumerate}
\item[1)] линейная регрессия,  $\sigma^{'} = \text{id}, \sigma = \text{id}, \w = \mathbf{I}_n $ ,
\item[2)] линейная регрессия и метод главных компонент, $\sigma^{'} = \text{id}, \w^\mathsf{T}\w = \mathbf{I}_n $ ,
\item[3)] линейная модель и автокодировщик, $\w^\mathsf{T}\w \neq \textbf{I}_n$,
\item[4)] линейная модель и стэк автокодировщиков, представимый в виде суперпозиции~\eqref{eq57},
\item[5)] двухслойная нейронная сеть,
\item[6)] глубокая нейронная сеть.
\end{enumerate}

\paragraph{Устойчивость~---}это минимум дисперсии функции ошибки \eqref{eq3}:
\begin{equation}\label{eq103}
\mathsf{D}(S) \rightarrow \min.
\end{equation}

your model in the class of models,
restrictions on the class of models,
the error function (and its inference) or a loss function, or a quality criterion,
cross-validation procedure,
restrictions to the solutions,
external (industrial) quality criteria,
the optimization statement as argmin.





















\paragraph{Выбор модели}

Задана выборка 
\begin{equation}\label{eq3}
(\xb_i,y_i),\quad \xb_i \in \mathbb{R}^n,\quad y_i\in \mathbb{R}^1,\quad i=1,\dots,m,
\end{equation}
где $\xb$~--- описание объекта, вектор из $n$ элементов признаков, $y$~--- зависимая переменная. Моделью называется отображение $f:(\xb,\wm)\mapsto y$. Требуется построить аппроксимирующую модель $f(\x)$ вида:
\begin{equation}\label{eq44}
f = \sigma_k\circ\underset{1\times1}{\wm_k^\mathsf{T}\sigmab_{k-1}}\circ\w_{k-1}\sigmab_{k-2}\circ\dots\circ\underset{n_2 \times 1}{\w_2\sigmab_1}\circ\underset{n_1 \times n}{\w_1}\underset{n \times 1}{\x}.
\end{equation}
Эта модель рассматривается как суперпозиция линейной модели, глубокой нейросети и автоэкнодера. Рассмотрим различные модели как частные случаи (\ref{eq44}).
\\
Линейная или логистическая регрессия и один нейрон~--- имеют вид
\begin{equation}\label{eq11}
f(\xb,\wm)=\sigmab(\wm^\mathsf{T}\xb),
\end{equation}
где $\sigmab$~---~ функция активации, непрерывная монотонная дифференцируемая функция~(\ref{eq49}),  $\wm$~---~ вектор параметров, $\xb$~---~объект, вектор с присоединенным элементом единица соответствующим аддитивному параметру $w_0$. При использовании линейной функции активации,  получаем линейную регрессию $f(\xb,\wm)=\wm^\mathsf{T}\xb.$

Такую функцию активации мы обозначим $\sigmab = \textbf{id} $. При использовании сигмоидной функции активации,  получаем модель логистической регрессии \begin{equation}\label{eq49}
f(\xb,\wm)=\sigma(\wm^\mathsf{T}\xb)=\frac{1}{1+\exp(-\wm^\mathsf{T}\xb)} .
\end{equation}

Двухслойная нейронная сеть, состоящая из линейной комбинации нейронов, однослойных нейронных сетей
\begin{equation}\label{eq12}
f(\xb,\wm)=\sigma^{(2)}\bigg(\sum_{i=1}^{n_2}w_i^{(2)}\cdot\sigma^{(1)}\Bigg(\sum_{i=1}^{n}w_{ij}^{(1)}x_j+w_{i0}^{(1)}\Bigg)+w_0^{(2)}\bigg) = \sigma\circ \wm^{\mathsf{T}}\sigmab\circ \w\xb.
\end{equation}

Метод главных компонент. Модель допускает вращения признакового пространства, то есть координаты (признаки) преобразовываются только с помощью поворотов:
\begin{equation}
\mathbf{h} = \w\x,
\end{equation}
где $\w$~---~ матрица поворота. Она ортогональна: 
\begin{equation}\label{eq51}
\w\w^\mathsf{T} = \mathbf{I}_n.
\end{equation}
Полученное пространство образов $\mathbf{h}$ называется скрытым. Происходит преобразование без потерь.

При удалении нескольких строк матрицы $\w$, например их число $u < n$,  полученный вектор $\mathbf{h}$ имеет размер $u \times 1$. Получается проекция $\mathbf{h}$ вектора $\mathbf{x}$. Согласно теореме Рао~С.Р.~\cite{fourth}, первые $u$ главных компонент восстанавливают $\mathbf{h}$ оптимальным способом,
\begin{equation}\label{eq45}
\textbf{r}(\textbf{x}) = \w^{\mathsf{T}}\mathbf{h}.
\end{equation}

Автокодировщик $\mathbf{h}$~---~ это монотонное нелинейное отображение входного вектора свободных переменных $\textbf{x} \in \mathbb{R}^n$ в скрытое представление $\mathbf{h} \in \mathbb{R}^{u}$ вида:
\begin{equation}\label{eq52}
\mathbf{h}(\x) = \sigmab(\underset{u \times n}{\w}\x + \mathbf{b}) .
\end{equation}
В случае $\sigmab = \textbf{id}$ и \eqref{eq51} автокодировщик тождественен методу главных компонент. Скрытое представление $\textbf{h}$ реконструирует вектор $\textbf{x}$ линейно:
\begin{equation}\label{eq53}
\mathbf{r}(\x) = \underset{n \times u}{\w{'}}\mathbf{h} + \wm_{0}^{'} .
\end{equation}


\section{Задача выбора оптимальной структуры модели}

Решается задача выбора оптимальной структуры модели
\begin{equation}\label{eq57}
f = \sigma_k\circ\boldsymbol{\Gamma}_k\otimes\underset{1\times1}{\wm_k^\mathsf{T}\sigmab_{k-1}}\circ\boldsymbol{\Gamma}_{k-1}\otimes\w_{k-1}\sigmab_{k-2}\circ\dots\circ\boldsymbol{\Gamma}_2\otimes\underset{n_2 \times 1}{\w_2\sigmab_1}\circ\boldsymbol{\Gamma}_1\otimes\underset{n_1 \times n}{\w_1}\underset{n \times 1}{\x},
\end{equation}
где $\boldsymbol{\Gamma}$~---~ матрица, задающая структуру модели; $\otimes$~--- адамарово произведение, определяющееся как поэлементное умножение. Если элемент $\gamma\in\{0,1\}$ матрицы $\boldsymbol{\Gamma}$ равен нулю, то соответствующий элемент матрицы параметров $\w$ обнуляется, и не участвует в работе модели. Множество индексов соответствующих ненулевым элементам матрицы $\boldsymbol{\Gamma}$ обозначается $\mathcal{A}$.  Требуется найти такое подмножество индексов $\mathcal{A}^{*}$, которое доставляет минимум функции:
\begin{equation}\label{eq46}
\mathcal{A}^{*} = \arg \underset{\mathcal{A} \subseteq \mathcal{I}}\min S(f_{\mathcal{A}}|\wm^*, \mathfrak{D}_\mathcal{C}),
\end{equation}
на разбиении выборки $\mathfrak{D}$, определенным множеством индексов $\mathcal{C}$. Здесь $\mathcal{I} = \mathcal{C}\sqcup \mathcal{L}$~---~ все индексы всех матриц $\boldsymbol{\Gamma}$. То есть требуется снизить число признаков и повысить устойчивость модели.

При этом параметры $\textbf{w}^*$ модели доставляют минимум ошибки:
\begin{equation}\label{eq102}
\textbf{w}^* = \arg \underset{\textbf{w}}\min S(\textbf{w}|\mathfrak{D}_\mathcal{L}, f_\mathcal{A}),
\end{equation}
на разбиении выборки, определенной множеством $\mathcal{L}$. Процедура разбиения описана в вычислительном эксперименте.



--------Здесь копипаст заканчивается ------------





Нейросеть является суперпозицией линейных моделей. В отличие от работы \cite{bib_4}, в данной работе предлагается заменить генетический алгоритм квадратичным программированием или градиентным спуском, построив построив новую функцию ошибки $S$.
\begin{equation}
S(\wm|\mathbf{\gamma},D) = S(\wm),
\end{equation}
\begin{equation}\label{eq11}
S = \lambda_1S_1+\dots+\lambda_kS_k = \mathbb{\lambda}^{\mathsf{T}}\mathbb{S}.
\end{equation}
Требуется создать каталог слагаемых функции ошибки.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|}
\hline
  Тип регуляризатора/слагаемого   & Роль  \\
  \hline
  $||\mathbf{y} - f(\w)||^2_2 = E_y$,   & Ошибка выхода нейронной сети \\
  \hline
  $||\xb-\mathbf{r}(\xb)||^2_2$ & Ошибка восстановления на каждом слое \\
  \hline
  $||\wm-\wm_0||_1$, $||\wm-\wm_0||^2_2$, & \\
  \hline
  $||\w-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от тождественного преобразования \\
  \hline
  $||\w\w^T-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от метода главных компонент \\
  \hline
\end{tabular}
\caption{Каталог слагаемых функции ошибки}
\end{center}
\end{table}

%\begin{enumerate}
%\item[1)]
%$||\mathbf{y} - f(\w)||^2_2 = E_y$,
%\item[2)]
%$||\xb-\mathbf{r}(\xb)||^2_2$,
%\item[3)]
%$||\wm-\wm_0||_1$,
%$||\wm-\wm_0||^2_2$,
%\item[4)]
%$||\w-\mathbf{I}||$,
%\item[5)]
%$||\w\w^T-\mathbf{I}||$,
%\end{enumerate}

Ошибка $E_k$ для каждого слоя будет разная. Пусть $\w$ - матрица параметров одного слоя нейросети, тогда одной строке из этой матрицы будет соответствовать один нейрон. Каждый слой нейросети рассматривается либо в качестве скрытого слоя нейросети, либо в качестве слоя автоенкодера. Тогда $E_x$ – ошибка восстановления на каждом слое. Для каждого слоя с номером $k$ задается ошибка этого слоя
\begin{equation}\label{eq4}
E_k = \lambda_xE_x(\xb-\mathbf{r})+\lambda_yE_y(y-f(\w)) +\lambda_wE(\w)_\text{Frobenius}. 
\end{equation}
Норма фробениуса представляет попарное расстояние от нейрона до нейрона. Требуется увеличить ее значение, чтобы нейроны на одном слое нейросети существенно отличались. Задается одна функция ошибки, с помощью которой будет оптимизироваться структура модели.
Функция ошибки включает штрафы на нейрон, штраф на то, что матрица одного слоя отличается от чистого метода главных компонент. Также требуется исследовать условия перехода от метода главных компонент к методу независимых компонент.

Функция ошибки содержит произведение $k$ элементов на количество ошибок, описанных в таблице, другими словами, на введенное число штрафов. Каждый слой нейросети содержит ошибку и маску $\Gamma$.
Рассморим один слой. Его качественная оценка состоит из слагаемых $||\w_2||, ||\w_1||, E_x, E_y$. А также $||\w||_{\text{Frobenius}}$, которая указывает на разнообразие нейронов в слое. Получается 5*4 слагаемых для 4 слоев нейросети. 
Часть слагаемых может быть вычислена и оптимизирована без основного цикла оптимизации. Автоенкодер требует оптимизации только при наличии 2 слоев и более, потому что если только один – то это метод главных компонент. 

В прошлой работе маска $\Gamma$ оптимизировалась генетическим алгоритмом. В текущей работе необхожимо сделать $\Gamma$ функцией от гиперпараметров, а конкретно от результатов анализа функции ошибки.
При оптимизации используется метод бустрепа и вместо каждого $\w$ оптимального имеется набор оптимальных $\w$, с помощью которых вычисляется ковариационная матрица $A$. Так как дисперсии складываются -  оцениваются var нейрона и var слоя. В прошлой работе в оптимизации не использавалась дисперсия функции ошибки.
Предлагается декомпозировать дисперсию и использовать ее для выбора структуры $\Gamma$. 


???Нейроны в слоях нейросети взаимозаменяемы и
от мультистарта до мультистарта они меняются местами. Желательно решить эту проблему в данной работе. 

Требуется создать расписание оптимизации параметров регуляризации $\lambda$. Требуется назначать лямбду в зависимости от номера итерации. Если оптимизация сети только началась, то нам важно подготовить выборку, чтобы на последнем слое нейросети она была простой. Поэтому требуется, чтобы начальные автоекнодеры работали хорошо. Для выбора $\lambda$ предполагается использовать эмпирический подход и исследовать различные техники, как например подходы для выбора шага обучения. Лямбда – точка парето оптимального фронта.

В прошлой работе поиск структуры работал через добавление или удаление элементов из структуры. Это решение по сути является лихорадочным метанием, которое работает как полный перебор. Требуется предложить стратегию направленного поиска. Такая стратегия, конечно, работает чуть хуже чем полный перебор. 
Алгоритм градиентного спуска сходится довольно быстро. Требуется исследовать возможность поиска оптимальной структуры через сложность. 




В работе \cite{wang2018learning} представлена таблица свойств различных регуляризаций. В работе рассматриваются нормы со степенью один и два, то есть метрики $L1$ и $L2$. Требуется исследовать свойства метрика с различными значениями степени. 




\section{Оптимизация структуры}
Иерархия структуры нейросети имеет следующий вид
$$\text{параметр} \prec \text{нейрон} \prec \text{слой} \prec \text{сеть}$$.
Оптимизация нейросети состоит из следующих шагов:
\begin{enumerate}
\item[1)]
Оптимизируем параметры,
\item[2)]
Оптимизируем структуру,
\end{enumerate}

\paragraph{Сложность.}
Введем отношение порядка $\succ$ на множестве значений сложности. Это отношение задается множеством параметров модели:
\begin{enumerate}[1)]
\item один параметр: $w\in \mathbb{R}^1 \succ w \in \lambda_1 [0,1] +\lambda_0 \succ w\in c +\lambda_0$,
\item вектор (нейрон): $\wm\in \mathbb{R}^n \succ \left\Vert \wm \right\Vert^2 =1 \succ \wm= \text{const}$,
\item матрица (слой): $\wvec\in \mathbb{R}^{c{\times}n} \succ \wvec^\mathsf{T}\wvec = \mathbf{I} \succ \wvec= \text{const}$.
\end{enumerate}

Устойчивость модели не возрастает с ее сложностью. Информативность каждого слоя, каждого нейрона, каждого элемента падает. 
Сложность задается как отношение порядка для множестве значений сложности. Структура которая включает 1 параметр проще структуры, которая включает два параметра. А две разные структуры с 1 параметров несравнимы с точки зрения сложности.
Следующие модели упорядочены по возрастанию сложности:
\begin{enumerate}[1)]
\item линейная регрессия,  $\sigma^{'} = \text{id}, \sigma = \text{id}, \w = \mathbf{I}_n $ ,
\item линейная регрессия и метод главных компонент, $\sigma^{'} = \text{id}, \w^\mathsf{T}\w = \mathbf{I}_n $ ,
\item линейная модель и автокодировщик, $\w^\mathsf{T}\w \neq \textbf{I}_n$,
\item линейная модель и стэк автокодировщиков, представимый в виде суперпозиции~\eqref{eq57},
\item двухслойная нейронная сеть,
\item глубокая нейронная сеть.
\end{enumerate}
\section{Расписание оптимизации}
Требуется задать раписание изменений параметра регуляризации $\mathbf{\lambda}$. Предлагается использовать экспертные методы определения расписания изменений $\lambda$. Например, задание $\lambda$ как функцию от номера итерации $t$ обучения нейросети $\lambda(t)=\frac{1}{t}$.

Рассматриваются два варианта использования этого параметра. Первый, обозначим его за $\lambda_S$ - это параметр регуляризации перед ошибкой каждого слоя нейросети \eqref{eq11}. С помощью него регуляризуются слои на разных стадиях обучения. Второй вариант $\lambda_{S_k}$ регулирует различные ошибки в функции ошибки одного слоя \eqref{eq4}. Использование этого параметра регуляризации позволяет нам изменять функцию слоя в структуре всей нейросети. Например, в функции \eqref{eq4} если занулить ошибки $E_y$ и $E_w$, оставив только ошибку $E_x$, то слой будет вести как автокодоривощик. Или наоборот, при слабой регуляризации параметра $E_y$ мы получим нейросеть.
Вообще говоря, $\lambda_{S_k}$ является матрицей $\lambda_{i,q}$, где $i$ соответствует номеру слоя, а $q$ - номеру слагаемого из функции ошибки определенного слоя
\[
\begin{bmatrix}
\lambda_{1,1}& \lambda_{1,2}&\dots&\lambda_{1,q}\\
\dots& \dots&\dots&\dots \\
\lambda_{i,1}& \lambda_{i,2}&\dots&\lambda_{i,q}
\end{bmatrix}
\]
В свою очередь $\lambda_k$ представляет собой вектор
\begin{equation}\label{eq4}
\mathbf{\lambda_k} = \left[\lambda_1,\lambda_2,\dots,\lambda_k \right]
\end{equation}
каждый элемент которого соответствует регуляризационному параметру отдельного слоя.
\section{Вычислительный эксперимент}
Исследуется процедура оптимизации структуры нейросети с сохранением качества аппроксимации. Структура оптимизируется с помощью регуляризации. Цель вычислительного эксперимента состоит в определении оптимальных значений гиперпараметров регуляризации, а так же исследовании зависимости точности,сложности и устойчивости модели от включенных типов регуляризации. Процедура построения модели включает в себя \mbox{следующие} шаги:
\begin{enumerate}
\item
Задание начальной суперпозиции. Для оптимизации параметров используется метод стохастического градиентного спуска.
\item
Оптимизируются гиперпарметры регуляризации.
\item
Для сравнения сложности структуры и исследования зависимости ошибки от сложности вводится отношение порядка на $\mathbf{\Gamma}$.
\item
Строятся графики зависимости ошибки от сложности модели.
\end{enumerate}

Рассматриваются регуляризации следующих типов:
\begin{enumerate}[label*=\arabic*.]
    \item
    Lasso или $L_1$ регуляризация вида:
    \[L_1(w) = \lambda_1\|w\|_1\]
    \item
    Штраф за количество слоев в нейронной сети:
    \[L_l(k) = \lambda_l \cdot k\]
    \item
    Штраф за неортогональность матрицы:
    \[L_o(W) = \lambda_o\|WW^T - I\|\]
    \item 
     \href{https://towardsdatascience.com/tikhonov-regularization-an-example-other-than-l2-8922ba51253d}{Несколько видов Тихоновской регуляризации}:
    \begin{enumerate}[label*=\arabic*.]
        \item
        Ridge или $L_2$ регуляризация вида:
        \[L_1(w) = \lambda_2\|w\|_2^2\]
        \item
        Штраф за частоту появления весов
        \[
        A = \frac{1}{3} \begin{bmatrix}
            \frac{2}{3}& \frac{2}{3} & 0 & 0 &0 &0& 0\\
            1 & 1 & 1 & 1& 0 & 0 & 0 \\
            0& 0& 1 & 1& 1 & 0 & 0 \\
            0& 0& 0 & 1& 1 & 1 & 0 \\
            0& 0& 0 & 0& 1 & 1 & 1\\
            0& 0& 0 & 0 & 0& \frac{2}{3}& \frac{2}{3}
        \end{bmatrix}
        \]
        \\
        
        \[L_{hf}(W) = \lambda_{hf} \|(I -A)W\|\]
        \item 
        Штраф за локальную разницу в весах
        \[
        B = \begin{bmatrix}
            -2& 2 & 0 & 0 & 0 &0 &0\\
            -1 & 0 & 1 & 0& 0 & 0 & 0 \\
            0& -1& 0 & 1& 0 & 0 & 0 \\
            0& 0& -1 & 0& 1 & 0 & 0 \\
            0& 0& 0 & -1& 0 & 1 & 0\\
            0& 0& 0 & 0 & 0& -2& 2
        \end{bmatrix}
        \]
        \\
        \[L_{ld}(W) = \lambda_{ld} \|BW\|\]
    \end{enumerate}
\end{enumerate}
Строятся графики зависимости ошибки от числа включенных оптимизаций. Гиперпараметры $\lambda$ свои для каждого слоя.
В сети используются активационные функции $ReLU$. Матрица гиперпараметров выглядит следующим образом
\[\{\mathbf{\lambda_1}, \mathbf{\lambda_l}, \mathbf{\lambda_o}, \mathbf{\lambda_2}, \mathbf{\lambda_{hf}}, \mathbf{\lambda_{ld}}\}\],
где:
\begin{itemize}
    \item 
    $\mathbf{\lambda_1}$ -- вектор весов регуляризации для Lasso.
    \item
    $\mathbf{\lambda_l}$ -- вектор весов регуляризации для количества слоев.
    \item
    $\mathbf{\lambda_o}$ -- вектор весов регуляризации ортогональности матрицы весов.
    \item
    $ \mathbf{\lambda_2}$ -- вектор весов регуляризации для Ridge.
    \item
    $\mathbf{\lambda_{hf}}$  -- вектор весов регуляризации частоты весов.
    \item
     $ \mathbf{\lambda_{ld}}$ -- вектор весов регуляризации локальной разницы в матрице весов.
\end{itemize}
Все эти вектора имеют размерность количества слоев.
\section{Заключение}

\end{comment}




\begin{comment}
\section{Оценка параметров в иерархической вероятностной модели}
В общем случае используется динамическое программирование, как последовательность функций Беллмана. Критерий обучения выглядит здесь
\begin{equation}\label{eq10}
\min J(\textbf{w}_t,\ t=1,\ldots,T|\textbf{r},\ c) = \sum_{t=2}^T(\textbf{w}_t - q\textbf{w}_{t-1})^{\mathsf{T}}\textbf{D}^{-1}_r(\textbf{w}_t - q\textbf{w}_{t-1}) + 2c\sum_{t=1}^T\sum_{i=1}^{N_t}\max(0,1 - y_{j,t}\textbf{w}^{\mathsf{T}}_t\textbf{x}_{j,t})
\end{equation}
Целевая функция \eqref{eq10} зависит от $T$ параметров $(\textbf{w}_1, \ldots, \textbf{w}_T)$, упорядоченных вдоль временной оси. Каждый параметр является $(n+1)$~--мерный вектором $\textbf{w}_t \in \mathbb{R}^{n+1}$ Критерий \eqref{eq10} может быть представлен как сумма элементарных функций, каждая из которых зависит от одного или двух векторов (соседних) $\textbf{w}_t$. Обозначим результат оптимизационной задачи как \begin{equation}
\widetilde{J}_t(\textbf{w}_t|\textbf{r},\ c) = \underset{\textbf{w}_s, s= 1, \ldots, t - 1}\min J_t(\textbf{w}_s, s = 1, \ldots, t|\textbf{r},c) = \underset{\textbf{w}_1,\ldots, \textbf{w}_{t-1}}\min J_t(\textbf{w}_1,\ldots,\textbf{w}_{t-1}, \textbf{w}_t|\textbf{r},c)
\end{equation}
Это функция Беллмана, полностью определенная на обучаемом наборе $\{(\textbf{X}_t,\textbf{Y}_t), t = 1,\ldots,T\}$ Главное свойство~--- реккурентное соотношение
\begin{equation}
\widetilde{J}_0(\textbf{w}_0|\textbf{r},\ c) \equiv 0, \textbf{w}_0 \in \mathbb{R}^{n+1},\ t = 0
\end{equation}
\begin{equation}
\widetilde{J}_t(\textbf{w}_t|\textbf{r},\ c) = 2c\sum_{j = 1}^{N_t}\max(0,1 - y_{j,t}\textbf{w}^{\mathsf{T}}_t\textbf{x}_{j,t}) + 
\end{equation}
\begin{equation}
+ \underset{\textbf{w}_{t-1} \in \mathbb{R}^{n+1}}\min\left[(\textbf{w}_t - q\textbf{w}_{t-1})^{\mathsf{T}}\textbf{D}^{-1}_r(\textbf{w}_t - q\textbf{w}_{t-1}) + \widetilde{J}_{t-1}(\textbf{w}_{t-1}|\textbf{r},c)\right], \textbf{w}_t \in \mathbb{R}^{n+1}, t = 1,\ldots, T.
\end{equation}
Тогда оптимальные параметры для гиперплоскости находятся как $(\hat{\textbf{w}}_{T}|\textbf{r}, q, c)$:
\begin{equation}
\hat{\textbf{w}}_{T}|\textbf{r}, q, c = \underset{\textbf{w}_T \in \mathbb{R}^{n+1}}\min \widetilde{J}_T(\textbf{w}_T|\textbf{r},q,c).
\end{equation}
Для достижения оптимальных скорости вычисления и использовании памяти стоит использовать аппроксимацию функции Беллмана. Тогда оптимизационная задача выпукла и принимает вид
\begin{equation}
\widetilde{J}_t'(\textbf{w}_t|\textbf{r},c) = (\textbf{w}_t - \widetilde{\textbf{w}}_t')^{\mathsf{T}}\widetilde{Q}'_t(\textbf{w}_t - \widetilde{\textbf{w}}'_t)
\end{equation}
С условиями
\begin{equation}
\begin{cases}
\widetilde{w}'_t = \argmin \widetilde{J}'_t(\textbf{w}_t|\textbf{r}, c), \\
\widetilde{Q}'_t = \nabla^2_{\textbf{w}_t}\widetilde{J}'_t(\textbf{w}_t|\textbf{r},c),& \textbf{w}_t = \widetilde{\textbf{w}}_t.
\end{cases}
\end{equation}

В случае задачи квадратичного динамического программирования постановка выглядит
\begin{equation}
J(\textbf{r}|\textbf{w}_t, t = 1,\ldots,T, \mu) = (T-1)\ln|\textbf{D}^{-1}_r| + \sum_{t=2}^T(\textbf{w}_t - q\textbf{w}_{t-1})^{\mathsf{T}}\textbf{D}^{-1}_r(\textbf{w}_t - q\textbf{w}_{t-1})
\end{equation}
\begin{equation}
\min 2\ln G(\textbf{r}|\mu)
\end{equation}

Если длина обучаемой последовательности достаточно большая $T \rightarrow \infty$, критерий принимает вид 
\begin{equation}
J(\textbf{r}|\textbf{w}_t, t=1,\ldots,T,\mu) \underset{T \rightarrow \infty}{\rightarrow} \sum_{i=1}^n\left[\left(T - 1 + \frac{1}{\mu}\right)\ln\frac{1}{r_i} + \frac{1}{r_i}\left(\sum_{t = 2}^T(\omega_{i,t})^2 + \frac{1}{\mu}\right)\right].
\end{equation}
В силу того, что сумма здесь выпуклая функция, и производные по $\frac{1}{r_i} = 0$, упрощается итоговая формула для решения $(\hat{\textbf{r}}|\textbf{w}_1, \ldots  ,\textbf{w}_T, \mu)$:
\begin{equation}
(\hat{r_i}|\textbf{w}_1, \ldots, \textbf{w}_T, \mu) = \frac{\sum_{t=2}^T(\omega_{i,t})^2 + \frac{1}{\mu}}{T - 1 + (1/\mu)},\, i=1,\ldots,n.  
\end{equation}
\end{comment}




\begin{comment}

Ошибка $E_k$ для каждого слоя будет разная. Пусть $\w$ - матрица параметров одного слоя нейросети, тогда одной строке из этой матрицы будет соответствовать один нейрон. Каждый слой нейросети рассматривается либо в качестве скрытого слоя нейросети, либо в качестве слоя автоенкодера. Тогда $E_x$ – ошибка восстановления на каждом слое. Для каждого слоя с номером $k$ задается ошибка этого слоя
\begin{equation}\label{eq4}
E_k = \lambda_xE_x(\xb-\mathbf{r})+\lambda_yE_y(y-f(\w)) +\lambda_wE(\w)_\text{Frobenius}. 
\end{equation}
Норма фробениуса представляет попарное расстояние от нейрона до нейрона. Требуется увеличить ее значение, чтобы нейроны на одном слое нейросети существенно отличались. Задается одна функция ошибки, с помощью которой будет оптимизироваться структура модели.
Функция ошибки включает штрафы на нейрон, штраф на то, что матрица одного слоя отличается от чистого метода главных компонент. Также требуется исследовать условия перехода от метода главных компонент к методу независимых компонент.

Функция ошибки содержит произведение $k$ элементов на количество ошибок, описанных в таблице, другими словами, на введенное число штрафов. Каждый слой нейросети содержит ошибку и маску $\Gamma$.
Рассморим один слой. Его качественная оценка состоит из слагаемых $||\w_2||, ||\w_1||, E_x, E_y$. А также $||\w||_{\text{Frobenius}}$, которая указывает на разнообразие нейронов в слое. Получается 5*4 слагаемых для 4 слоев нейросети. 
Часть слагаемых может быть вычислена и оптимизирована без основного цикла оптимизации. Автоенкодер требует оптимизации только при наличии 2 слоев и более, потому что если только один – то это метод главных компонент. 
\end{comment}
\begin{comment}



В прошлой работе маска $\Gamma$ оптимизировалась генетическим алгоритмом. В текущей работе необхожимо сделать $\Gamma$ функцией от гиперпараметров, а конкретно от результатов анализа функции ошибки.
При оптимизации используется метод бустрепа и вместо каждого $\w$ оптимального имеется набор оптимальных $\w$, с помощью которых вычисляется ковариационная матрица $A$. Так как дисперсии складываются -  оцениваются var нейрона и var слоя. В прошлой работе в оптимизации не использавалась дисперсия функции ошибки.
Предлагается декомпозировать дисперсию и использовать ее для выбора структуры $\Gamma$. 

\end{comment}

\begin{comment}

???Нейроны в слоях нейросети взаимозаменяемы и
от мультистарта до мультистарта они меняются местами. Желательно решить эту проблему в данной работе. 

\end{comment}