\documentclass[12pt, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\newcommand{\hdir}{.}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{euscript}
\usepackage{upgreek}
\usepackage{array}
%\usepackage{theorem}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{caption}
\usepackage{color}
\usepackage{url}
\usepackage{cite}
\usepackage{geometry}
\usepackage{tikz,fullpage}
\usepackage{enumerate}
\usepackage{autonum}
\usepackage{enumitem}%
%\usepackage[unicode, pdftex]{hyperref}
\usepackage{comment}


\newcommand{\bmatr}{{\mathbf{B}}}
\newcommand{\cmatr}{{\mathbf{C}}}
\newcommand{\hmatr}{{\mathbf{H}}}
\newcommand{\fmatr}{{\mathbf{F}}}
\newcommand{\mmatr}{{\mathbf{M}}}
\newcommand{\xmatr}{{\mathbf{X}}}
\newcommand{\pmatr}{{\mathbf{P}}}
\newcommand{\xmatrt}{{\tilde{\mathbf{X}}}}
\newcommand{\imatr}{{\mathbf{I}}}
\newcommand{\vmatr}{{\mathbf{V}}}
\newcommand{\wmatr}{{\mathbf{W}}}
\newcommand{\umatr}{{\mathbf{U}}}
\newcommand{\zmatr}{{\mathbf{Z}}}
\newcommand{\zmatrt}{{\tilde{\mathbf{Z}}}}
\newcommand{\Tmatr}{\mathbf{T}}
\newcommand{\lambdamatr}{{\mathbf{\Lambda}}}
\newcommand{\phimatr}{\mathbf{\Phi}}
\newcommand{\sigmamatr}{\mathbf{\Sigma}}
\newcommand{\thetamatr}{\boldsymbol{\Theta}}

\newcommand{\ab}{{\mathbf{a}}}
\newcommand{\bb}{{\mathbf{b}}}
\newcommand{\cb}{{\mathbf{c}}}
\newcommand{\db}{{\mathbf{d}}}
\newcommand{\eb}{{\mathbf{e}}}
\newcommand{\fb}{{\mathbf{f}}}
\newcommand{\gb}{{\mathbf{g}}}
\newcommand{\hb}{{\mathbf{h}}}
\newcommand{\mb}{{\mathbf{m}}}
\newcommand{\pb}{{\mathbf{p}}}
\newcommand{\qb}{{\mathbf{q}}}
\newcommand{\rb}{{\mathbf{r}}}
\newcommand{\tb}{{\mathbf{t}}}
\newcommand{\ub}{{\mathbf{u}}}
\newcommand{\vb}{{\mathbf{v}}}
\newcommand{\wb}{{\mathbf{W}}}
\newcommand{\xb}{{\mathbf{x}}}
\newcommand{\xt}{{\tilde{x}}}
\newcommand{\xbt}{\tilde{{\mathbf{x}}}}
\newcommand{\yb}{{\mathbf{y}}}
\newcommand{\zb}{{\mathbf{z}}}
\newcommand{\zt}{{\tilde{z}}}
\newcommand{\zbt}{{\tilde{\mathbf{z}}}}
\newcommand{\mub}{{\boldsymbol{\mu}}}
\newcommand{\alphab}{{\boldsymbol{\alpha}}}
\newcommand{\thetab}{{\boldsymbol{\theta}}}
\newcommand{\iotab}{\boldsymbol{\iota}}
\newcommand{\zetab}{\boldsymbol{\zeta}}
\newcommand{\xib}{\boldsymbol{\xi}}
\newcommand{\xibt}{\tilde{\boldsymbol{\xi}}}
\newcommand{\xit}{\tilde{\xi}}
\newcommand{\betab}{{\boldsymbol{\beta}}}
\newcommand{\phib}{{\boldsymbol{\phi}}}
\newcommand{\psib}{{\boldsymbol{\psi}}}
\newcommand{\gammab}{{\boldsymbol{\gamma}}}
\newcommand{\lambdab}{{\boldsymbol{\lambda}}}
\newcommand{\varepsilonb}{{\boldsymbol{\varepsilon}}}
\newcommand{\pib}{{\boldsymbol{\pi}}}
\newcommand{\sigmab}{{\boldsymbol{\sigma}}}

\newenvironment{comment}{}{}



\newcommand{\scl}{s_{\mathsf{c}}}
\newcommand{\shi}{s_{\mathsf{h}}}
\newcommand{\shib}{\mathbf{s}_{\mathsf{h}}}
\newcommand{\MOD}{M}
\newcommand{\entr}{\mathsf{H}}
\newcommand{\REG}{\Omega}
\newcommand{\Mquol}{V}
\newcommand{\prob}{p}
\newcommand{\expec}{\mathsf{E}}

\newcommand{\xo}{{\overline{x}}}
\newcommand{\Xo}{{\overline{x}}}
\newcommand{\yo}{{\overline{y}}}

\newcommand{\xbo}{{\overline{\mathbf{x}}}}
\newcommand{\Xbo}{{\overline{\mathbf{X}}}}

\newcommand{\Amc}{{\mathcal{A}}}
\newcommand{\Bmc}{{\mathcal{B}}}
\newcommand{\Cmc}{{\mathcal{C}}}
\newcommand{\Jmc}{{\mathcal{J}}}
\newcommand{\Imc}{{\mathcal{I}}}
\newcommand{\Kmc}{{\mathcal{K}}}
\newcommand{\Lmc}{{\mathcal{L}}}
\newcommand{\Mmc}{{\mathcal{M}}}
\newcommand{\Nmc}{{\mathcal{N}}}
\newcommand{\Pmc}{{\mathcal{P}}}
\newcommand{\Tmc}{{\mathcal{T}}}
\newcommand{\Vmc}{{\mathcal{V}}}
\newcommand{\Wmc}{{\mathcal{W}}}
\newcommand{\Smi}{{\mathcal{S}}}

\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}
\newcommand{\deist}{\mathbb{R}}
\newcommand{\ebb}{\mathbb{E}}

\newcommand{\Amatr}{\mathbf{A}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Umatr}{\mathbf{U}}
\newcommand{\zetavec}{\boldsymbol{\zeta}}

\newcommand{\M}{\mathbf{M}}
\newcommand{\x}{{\mathbf{x}}}
\newcommand{\z}{{\mathbf{z}}}
\newcommand{\ical}{{\mathcal{I}}}
\newcommand{\tvec}{{\mathbf{t}}}
\newcommand{\xvec}{{\mathbf{x}}}
\newcommand{\zvec}{{\mathbf{z}}}
\newcommand{\bvec}{{\mathbf{b}}}
\newcommand{\qvec}{{\mathbf{z}}}
\newcommand{\pvec}{{\mathbf{p}}}
\newcommand{\wvec}{{\mathbf{W}}}
\newcommand{\rvec}{{\mathbf{r}}}
\newcommand{\thetavec}{{\mathbf{\theta}}}
\newcommand{\y}{{\mathbf{y}}}
\newcommand{\g}{{\mathbf{g}}}
\newcommand{\w}{{\mathbf{W}}}
\newcommand{\wm}{{\mathbf{w}}}
\newcommand{\m}{{\mathbf{m}}}


\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Определение}
\DeclareMathOperator*{\argmin}{arg\,min}
\graphicspath{ {pics/} }
%\graphicspath{ {fig/} }
\begin{document}
\selectlanguage{russian}
\title{\textbf{Аддитивная регуляризация и ее метапараметры при выборе структуры сетей глубокого обучения}\thanks{Работа выполнена при поддержке РФФИ (проекты 19-07-1155, 19-07-0885) и правительства РФ (соглашение 05.Y09.21.0018). Настоящая статья содержит результаты проекта <<Статистические методы машинного обучения>>, выполняемого В~рамках реализации Программы Центра компетенций Национальной технологической инициативы <<Центр хранения и анализа больших данных>>, поддерживаемого Министерством науки и высшего образования Российской Федерации по Договору МГУ им.\,М.\,В.\,Ломоносова  с Фондом поддержки проектов Национальной технологической инициативы от 11.12.2018 № 13/1251/2018.}}
\date{}
\maketitle

\begin{center}
\bf
М.\,С.~Потанин\footnote{Московский физико-технический институт, mark.potanin@phystech.edu}, К.\,О.~Вайсер\footnote{Московский физико-технический институт, vajser.ko@phystech.edu},  В.\,В.\,Стрижов\footnote{Вычислительный центр имени А.\,А.\,Дородницына Федерального исследовательского центра <<Информатика и управление>> Российской академии наук; Московский физико-технический институт, strijov@phystech.edu}
\end{center}

\begin{abstract}	
Решается задача выбора модели глубокого обучения и оптимизация функции ошибки, включающей аддитивную регуляризацию. Моделью является суперпозиция обобщенных линейных моделей, элементами которой являются автоэнкодер и нейросеть. Под сложностью модели понимаются число и абсолютное значение параметров модели. Исследуются свойства алгоритма оптимизации метапараметров аддитивной регуляризации.  Исследуются зависимости точности, сложности и устойчивости модели от метапарметров аддитивной регуляризации.
\end{abstract}

{\bf Ключевые слова:} автокодировщик; нейронные сети; структура; аддитивная регуляризация.

\setcounter{secnumdepth}{0}
\section{Введение}


В данной работе рассматривается влияние способа построения функции ошибки аддитивной регуляризации на выбор структуру сети глубокого обучения. Задачей такой сети является как можно более точная аппроксимация исходной неизвестной зависимости между векторами из признакового пространства и целевыми объектами.  Моделью называется отображение $f:(\xb,\wm)\mapsto y$.В общем виде модель глубокого обучения выглядит как \eqref{eq:model}. Такая модель может содержать большое число слоев и нейронов, что приводит к понятию сложности модели, которая понимается как число параметров модели. Предполагается, что чем сложнее модель, тем выше у нее точность аппроксимациии. Однако, увеличение сложности модели приводит к снижению ее устойчивости, то есть зависимости результата от изменения начальных данных (сюда ссылку про это). Предлагается разработать метод, позволяющий понизить сложность модели при сохранении ее точности.
Для решения этой задачи предлагается использовать аддитивную регуляризацию. 

Задачи, решение которых не существует, не единственно или не устойчиво, принято называть \textit{некорректно поставленными по Адамару.(Тихонов 68 год ссылка)} 
\begin{definition}%{Regularization}
Регуляризация~---метод решения задач, в котором для выбора оптимального решения задаются дополнительные критерии
оптимальности, учитывающие специфические требования решаемой задачи и называемые регуляризаторами. 
\end{definition}
\begin{definition}%{ Additive Regularization}
Аддитивная регуляризация~---это вид регуляризации, основанный на оптимизации взвешенной суммы критериев регуляризации. 
\end{definition}
Регуляризация предотвращает [ссылка] ситуацию, когда параметры становятся константными и не изменяются далее. Также она повышает устойчивость весов в случае мультиколлинеарности,
способствует повышению обобщающей способности алгоритма и снижению риска переобучения. [ссылка]

В качестве объекта исследования выступает способ построения функции ошибки. Она будет иметь вид \eqref{eq:error_function}. Исследуется влияние слагаемых \eqref{regularization_sum} на сложность и точность модели. Для этого необходимо составить расписание оптимизации метапараметров $\lambda$ аддитивной регуляризации.
\begin{definition}%{Metaparameters}
Метапараметры ~--- настраиваемые параметры функции ошибки, используемые для оптимизации параметров модели.
\end{definition}
В отличие от регуляризации для линейных моделей, метапарамеры аддитивной регуляризации назначаются для каждого из слоев нейронной сети. 
\begin{definition}%{Hyperparameters}
Гиперпараметры -- экспертно заданные параметры, используемые для оптимизации параметров модели и метапараметров.
\end{definition}
\begin{equation}\label{regularization_sum}
\sum\limits_{i=1}^r \mathbf{\lambda}_i^T\mathbb{S}_i    
\end{equation}

Элемент функции ошибки с аддитивной регуляризацией имеет следующую структуру:
\[\lambda \|L\wm\|_2^2,\]
где $\lambda$ ~--- метапараметр модели, $L$ ~--- гиперпараметр модели, $\wm$ ~--- параметр модели. Варьирование значений метапараметров добавляет и удаляет из рассмотрения различные регуляризаторы, причем индивидуально для каждого слоя нейросети.

Алгоритм оптимизации параметров модели состоит из:
\begin{enumerate}
    \item
    экспертное задание гиперпараметров модели.
    \item 
    оптимизация параметров модели.
    \item
    оптимизация метапараметров модели.
\end{enumerate}
Параметры модели оптимизируются алгоритм обратного распространения ошибки. Оптимизация метапараметров происходит с помощью генетического алгоритма.



\section{Обзор литературы}
Помимо аддитивной регуляризации были разработаны и другие методы. В статье \cite{chernousova2014linear} предлагается эффективный алгоритм вычисления ошибки при использовании контроля по отдельным объектам.\\
Задана выборка :
\begin{equation}\label{f1}
\left\{\left(\mathbf{x}_{j}, y_{j}\right), j=1, \ldots, N\right\}, \mathbf{x}_{j}=\left(x_{1 j} \cdots x_{n j}\right)^{T} \in \mathbb{R}^{n}, y_{j} \in \mathbb{R}
\end{equation}
\begin{equation}
\sum_{j=1}^{N} \mathbf{x}_{j}=\mathbf{0}, \sum_{j=1}^{N} y_{j}=0, \frac{1}{N} \sum_{j=1}^{N} x_{i j}^{2}=1, i \in I=\{1, \ldots, n\},
\end{equation}
 Задача -- минимизировать вектор \textbf{a}, параметры регрессии $\hat{y}(\mathbf{x})=\mathbf{a}^{T} \mathbf{x}$:
\begin{equation}
\begin{aligned} J_{\mathrm{NEN}}\left(\mathbf{a} | \lambda_{1}, \lambda_{2}\right) &=\lambda_{2} \sum_{i=1}^{n} a_{i}^{2}+\lambda_{1} \sum_{i=1}^{n}\left|a_{i}\right|+\sum_{j=1}^{N}\left(y_{j}-\sum_{i=1}^{n} a_{i} x_{i j}\right)^{2} \\ &=\lambda_{2} \mathbf{a}^{T} \mathbf{a}+\lambda_{1}\|\mathbf{a}\|_{1}+(\mathbf{y}-\mathbf{X} \mathbf{a})^{T}(\mathbf{y}-\mathbf{X} \mathbf{a}) \rightarrow \min (\mathbf{a}) \\ \mathbf{y}=&\left(y_{1} \cdots y_{N}\right) \in \mathbb{R}^{N}, \mathbf{X}=\left(\mathbf{x}_{1} \cdots \mathbf{x}_{N}\right)^{T}(N \times n) \\ \hat{\mathbf{a}}_{\lambda_{1}, \lambda_{2}} &=\left(\hat{a}_{i, \lambda_{1}, \lambda_{2}}, i \in I\right)=\arg \min J_{\mathrm{NEN}}\left(\mathbf{a} | \lambda_{1}, \lambda_{2}\right) \in \mathbb{R}^{n} \end{aligned}
\end{equation}

Предлагается следующий подход:
\begin{equation}
\begin{array}{l}{J_{\mathrm{EN}}\left(\mathbf{a} | \lambda_{1}, \lambda_{2}\right)=\lambda_{2} \sum_{i=1}^{n}\left(a_{i}-a_{i}^{*}\right)^{2}+\lambda_{1} \sum_{i=1}^{n}\left|a_{i}\right|+\sum_{j=1}^{N}\left(y_{j}-\sum_{i=1}^{n} a_{i} x_{i j}\right)^{2}} \\ {\quad=\lambda_{2}\left(\mathbf{a}-\frac{1}{N} \mathbf{X}^{T} \mathbf{y}\right)^{T}\left(\mathbf{a}-\frac{1}{N} \mathbf{X}^{T} \mathbf{y}\right)+\lambda_{1}\|\mathbf{a}\|_{1}+(\mathbf{y}-\mathbf{X} \mathbf{a})^{T}(\mathbf{y}-\mathbf{X} \mathbf{a}) \rightarrow \min (\mathbf{a})}\end{array}
\end{equation}
\begin{equation}
\hat{\mathbf{a}}_{\lambda_{1}, \lambda_{2}}=\left(\hat{a}_{i, \lambda_{1}, \lambda_{2}}, i \in I\right)=\arg \min J_{\mathrm{EN}}\left(\mathbf{a} | \lambda_{1}, \lambda_{2}\right) \in \mathbb{R}^{n},
\end{equation}
где $\mathbf{a}^{*}=(1 / N) \mathbf{X}^{T} \mathbf{y}$. Таким образом, можно переписать задачу в следующем виде:
\begin{equation}
\frac{\lambda_{1}}{1+\lambda_{2} / N}\|\mathbf{a}\|_{1}+\left[\mathbf{a}^{T} \frac{\mathbf{X}^{T} \mathbf{X}+\lambda_{2} \mathbf{I}}{1+\lambda_{2} / N} \mathbf{a}-2 \mathbf{y}^{T} \mathbf{X} \mathbf{a}\right] \rightarrow \min (\mathbf{a}),
\end{equation}
Вводится разбиение множества параметров по знаку:
\begin{equation}
\left\{\begin{array}{l}
{\hat{I}_{11}^{-}, \lambda_{2}=\left\{i \in I: \hat{a}_{i, \lambda_{1}, \lambda_{2}}<0\right\}} \\
{\hat{I}_{1_{1}, \lambda_{2}}^{0}=\left\{i \in I: \hat{a}_{i, \lambda_{1}, \lambda_{2}}=0\right\}, I=\hat{I}_{\lambda_{1}, \lambda_{2}}^{-} \cup \hat{I}_{\lambda_{1}, \lambda_{2}}^{0} \cup \hat{I}_{\lambda_{1} \lambda_{2}}^{+}} \\
{\hat{I}_{\lambda_{1}^{+}, \lambda_{2}}^{+}=\left\{i \in I: \hat{a}_{i, \lambda_{1}, \lambda_{2}}>0\right\}}
\end{array}\right.
\end{equation}.
После некоторых преобразований:
\begin{equation}
\begin{array}{c}
{\hat{S}_{\mathrm{LOO}}\left(\lambda_{1}, \lambda_{2}\right)=\frac{1}{N} \sum_{k=1}^{N}\left(\hat{\delta}_{k, \lambda_{1}, \lambda_{2}}^{(k)}\right)^{2}} \\
{\hat{\delta}_{k, \lambda_{1}, \lambda_{2}}^{(k)}=y_{k}-\hat{y}_{k, \lambda_{1}, \lambda_{2}}^{(k)}=y_{k}-\tilde{\mathbf{x}}_{k}^{T} \hat{\mathbf{a}}_{\lambda_{1}, \lambda_{2}}^{(k)}}
\end{array}
\end{equation}
и
\begin{equation}
\hat{S}_{\mathrm{LOO}}^{\mathrm{EN}}\left(\lambda_{1}, \lambda_{2}\right)=\frac{1}{N} \sum_{k=1}^{N}\left(\frac{\hat{\delta}_{k, \lambda_{1}, \lambda_{2}}+\frac{1}{N-1} \lambda_{2}\left(y_{k} q_{k, \lambda_{1}, \lambda_{2}}-h_{k, \lambda_{1}, \lambda_{2}}\right)}{1-q_{k, \lambda_{1}, \lambda_{2}}}\right)^{2}(\text {ElasticNet})
\end{equation}
\begin{equation}
\begin{aligned}
&q_{k, \lambda_{1}, \lambda_{2}}=\tilde{\mathbf{x}}_{k}^{T}\left(\tilde{\mathbf{X}}_{\lambda_{1}, \lambda_{2}}^{T} \tilde{\mathbf{X}}_{\lambda_{1}, \lambda_{2}}+\lambda_{2} \tilde{\mathbf{I}}_{\hat{n}_{\lambda_{1}, \lambda_{2}}}\right)^{-1} \tilde{\mathbf{x}}_{k}\\
&h_{k, \lambda_{1}, \lambda_{2}}=\tilde{\mathbf{x}}_{k}^{T}\left(\tilde{\mathbf{X}}_{\lambda_{1}, \lambda_{2}}^{T} \tilde{\mathbf{X}}_{\lambda_{1}, \lambda_{2}}+\lambda_{2} \tilde{\mathbf{I}}_{\hat{n}_{\lambda_{1}, \lambda_{2}}}\right)^{-1} \tilde{\mathbf{a}}^{*}
\end{aligned}
\end{equation}

Для практического использования модель должна не только достигать высокого качества предсказаний, но и быть интерпретируемой. Например, исследования показали, что в области медицины предпочитаются модели основанные на деревьях решений, из-за того, что можно проследить,на основании каких правил строились результаты работы модели. Тем не менее, возможность узнать причины предсказаний модели не являтся основным критерием применимости модели. Если эти причины очень сильно расходятся со здравым смыслом и устоявшимися законами данной области, то наврял ои кто-то решиит прислушаться к такой модели. Эти два качества называются интерпретируемость и правдоподобность модели. В работе \cite{wang2018learning} авторы рассматривают новый способ регуляризации EYE (expert yielded estimates), который включает в себя экспертные знания об отношениях между признаками и зависимой переменной. Авторы расматривают задачу минимизации эмпирического риска
\begin{equation}\label{eq1}
\hat{\mathbf{\w}} = \argmin\text{S}(\w,\X,\mathbf{y})+n\lambda\mathcal{J}(\w,\mathbf{\Gamma})
\end{equation}
в которой минимизируется сумма функции ошибки и регуляризации $\mathcal{J}$. Имеется множества признаков $\mathcal{D}$, из которых для множества $\mathcal{K}\subseteq\mathcal{D}$ имеется дополнительная информация о том, что эти признаки являются важными в рассматриваемой экспертной области. Следовательно для $\hat{\mathbf{\w}}_{\mathcal{D}\setminus\mathcal{K}}$ требуется разреженность, а для $\hat{\mathbf{\w}}_{\mathcal{K}}$ - нет. Базовый или <<наинвый>> подход, используемый авторами, заключается в использовании $L1$ и $L2$ регуляризаций, тогда регуляризационный член имеет вид
\begin{equation}\label{eq2}
\mathcal{J} = (1-\beta)||\mathbf{\Gamma}\odot\w||^2_2+\beta||(1-\mathbf{\Gamma})\odot\w||_1,
\end{equation}
где параметр $\beta$ контролирует баланс между признаками из $\mathcal{K}$ и $\mathcal{D}\setminus\mathcal{K}$. Предлагаемое авторами решение имеет следующий вид
\begin{equation}\label{eq3}
\mathcal{J} = ||(1-\mathbf{\Gamma})\odot\w||_1 + \sqrt{ ||(1-\mathbf{\Gamma})\odot\w||^2_1+||\mathbf{\Gamma}\odot\w||^2_2},
\end{equation}
Его  использование в задачах стратификации риска пациентов позволило получить модели, в которых основные используемые признаки сильно пересекались с факторами, которые считаются значимыми в медицинской среде. При э том удалось сохранить высокое качество предсказания.


\begin{comment}
\section{Оценка параметров в иерархической вероятностной модели}
В общем случае используется динамическое программирование, как последовательность функций Беллмана. Критерий обучения выглядит здесь
\begin{equation}\label{eq10}
\min J(\textbf{w}_t,\ t=1,\ldots,T|\textbf{r},\ c) = \sum_{t=2}^T(\textbf{w}_t - q\textbf{w}_{t-1})^{\mathsf{T}}\textbf{D}^{-1}_r(\textbf{w}_t - q\textbf{w}_{t-1}) + 2c\sum_{t=1}^T\sum_{j=1}^{N_t}\max(0,1 - y_{j,t}\textbf{w}^{\mathsf{T}}_t\textbf{x}_{j,t})
\end{equation}
Целевая функция \eqref{eq10} зависит от $T$ параметров $(\textbf{w}_1, \ldots, \textbf{w}_T)$, упорядоченных вдоль временной оси. Каждый параметр является $(n+1)$~--мерный вектором $\textbf{w}_t \in \mathbb{R}^{n+1}$ Критерий \eqref{eq10} может быть представлен как сумма элементарных функций, каждая из которых зависит от одного или двух векторов (соседних) $\textbf{w}_t$. Обозначим результат оптимизационной задачи как \begin{equation}
\widetilde{J}_t(\textbf{w}_t|\textbf{r},\ c) = \underset{\textbf{w}_s, s= 1, \ldots, t - 1}\min J_t(\textbf{w}_s, s = 1, \ldots, t|\textbf{r},c) = \underset{\textbf{w}_1,\ldots, \textbf{w}_{t-1}}\min J_t(\textbf{w}_1,\ldots,\textbf{w}_{t-1}, \textbf{w}_t|\textbf{r},c)
\end{equation}
Это функция Беллмана, полностью определенная на обучаемом наборе $\{(\textbf{X}_t,\textbf{Y}_t), t = 1,\ldots,T\}$ Главное свойство~--- реккурентное соотношение
\begin{equation}
\widetilde{J}_0(\textbf{w}_0|\textbf{r},\ c) \equiv 0, \textbf{w}_0 \in \mathbb{R}^{n+1},\ t = 0
\end{equation}
\begin{equation}
\widetilde{J}_t(\textbf{w}_t|\textbf{r},\ c) = 2c\sum_{j = 1}^{N_t}\max(0,1 - y_{j,t}\textbf{w}^{\mathsf{T}}_t\textbf{x}_{j,t}) + 
\end{equation}
\begin{equation}
+ \underset{\textbf{w}_{t-1} \in \mathbb{R}^{n+1}}\min\left[(\textbf{w}_t - q\textbf{w}_{t-1})^{\mathsf{T}}\textbf{D}^{-1}_r(\textbf{w}_t - q\textbf{w}_{t-1}) + \widetilde{J}_{t-1}(\textbf{w}_{t-1}|\textbf{r},c)\right], \textbf{w}_t \in \mathbb{R}^{n+1}, t = 1,\ldots, T.
\end{equation}
Тогда оптимальные параметры для гиперплоскости находятся как $(\hat{\textbf{w}}_{T}|\textbf{r}, q, c)$:
\begin{equation}
\hat{\textbf{w}}_{T}|\textbf{r}, q, c = \underset{\textbf{w}_T \in \mathbb{R}^{n+1}}\min \widetilde{J}_T(\textbf{w}_T|\textbf{r},q,c).
\end{equation}
Для достижения оптимальных скорости вычисления и использовании памяти стоит использовать аппроксимацию функции Беллмана. Тогда оптимизационная задача выпукла и принимает вид
\begin{equation}
\widetilde{J}_t'(\textbf{w}_t|\textbf{r},c) = (\textbf{w}_t - \widetilde{\textbf{w}}_t')^{\mathsf{T}}\widetilde{Q}'_t(\textbf{w}_t - \widetilde{\textbf{w}}'_t)
\end{equation}
С условиями
\begin{equation}
\begin{cases}
\widetilde{w}'_t = \argmin \widetilde{J}'_t(\textbf{w}_t|\textbf{r}, c), \\
\widetilde{Q}'_t = \nabla^2_{\textbf{w}_t}\widetilde{J}'_t(\textbf{w}_t|\textbf{r},c),& \textbf{w}_t = \widetilde{\textbf{w}}_t.
\end{cases}
\end{equation}

В случае задачи квадратичного динамического программирования постановка выглядит
\begin{equation}
J(\textbf{r}|\textbf{w}_t, t = 1,\ldots,T, \mu) = (T-1)\ln|\textbf{D}^{-1}_r| + \sum_{t=2}^T(\textbf{w}_t - q\textbf{w}_{t-1})^{\mathsf{T}}\textbf{D}^{-1}_r(\textbf{w}_t - q\textbf{w}_{t-1})
\end{equation}
\begin{equation}
\min 2\ln G(\textbf{r}|\mu)
\end{equation}

Если длина обучаемой последовательности достаточно большая $T \rightarrow \infty$, критерий принимает вид 
\begin{equation}
J(\textbf{r}|\textbf{w}_t, t=1,\ldots,T,\mu) \underset{T \rightarrow \infty}{\rightarrow} \sum_{i=1}^n\left[\left(T - 1 + \frac{1}{\mu}\right)\ln\frac{1}{r_i} + \frac{1}{r_i}\left(\sum_{t = 2}^T(\omega_{i,t})^2 + \frac{1}{\mu}\right)\right].
\end{equation}
В силу того, что сумма здесь выпуклая функция, и производные по $\frac{1}{r_i} = 0$, упрощается итоговая формула для решения $(\hat{\textbf{r}}|\textbf{w}_1, \ldots  ,\textbf{w}_T, \mu)$:
\begin{equation}
(\hat{r_i}|\textbf{w}_1, \ldots, \textbf{w}_T, \mu) = \frac{\sum_{t=2}^T(\omega_{i,t})^2 + \frac{1}{\mu}}{T - 1 + (1/\mu)},\, i=1,\ldots,n.  
\end{equation}
\end{comment}

Регуляризация $L2$ работает с помощью установления баланса между смещением и дисперсией. Но ее недостаток в том, что она не может создать разреженную модель, так как сохраняет все множество признаков. Другой вид регуляризации $L1$ был предложен в \cite{tibshirani1996regression}, и он предлагает автоматический выбор переменных. Но он также имеет несколько недостатков, среди которых:
\begin{enumerate}
\item[1)]
Если обозначить за $p$ число независимых переменных, а за $n$ количество объектов в выборке, то в случае $p>n$ lasso регуляризация выбирает максимально $n$ независимых переменных из множества.
\item[2)]
При наличии групп сильно скоррелированных переменных, lasso регуляризация выбирает только одну переменную из группы, причем не обращая внимания какую именно.
\item[3)]
В случае $n>p$ и наличии высокой корреляции между переменными было эмпирически показано, что ridge регрессия работает намного лучше lasso.
\end{enumerate}
Таким образом, пукнты 1) и 2) делают lasso неприменимой техникой в некоторых задачах, где требуется отбор признаков. Перечисленные проблемы решаются с помощью другой техники регуляризации elastic net \cite{zou2005regularization}, которая позволяет производить автоматический отбор переменных, регулировать их веса, а так же выбирать группы коррелярующих признаков. Метод регуляризации elastic net представляет собой добавление в функцию ошибки двух дополнительных слагаемых
\begin{equation}
S(\lambda_1,\lambda_2,\wm) = |\yb-\xmatr\wm|^2+\lambda_2|\wm|^2+\lambda_1|\wm|_1
\end{equation}

Некоторым обобщением elastic net является регуляризация Support Features Machine (SFM). Задается в виде
\begin{equation}
\underset{\mathbf{w},\mathbf{w_0}}\min\quad C\sum_{i=1}^l(1 - M_i(\mathbf{w},\mathbf{w_0}))_+ + \sum_{j=1}^nR_{\mu}(w_j),
\end{equation}
\begin{equation}
R_{\mu}(w_j) = \begin{cases}
2\mu|w_j|,& |w_j| \leq \mu, \\
\mu^2 + w_j^2,& |w_j| \geq \mu. \\
\end{cases}
\end{equation}
Отбор признаков осуществляется с помощью параметра селективности $\mu$. Также присутствует эффект группировки. Шумовые признаки ($|w_j| < \mu$) подавляются как и в Lasso, а значимые зависимые признаки группируются также как и в elastic net.
На нее похож такой метод как Relevance Features Machine (RFM). Задается в виде
\begin{equation}
\underset{\mathbf{w},\mathbf{w_0}}\min\quad C\sum_{i=1}^l(1 - M_i(\mathbf{w},\mathbf{w_0}))_+ + \sum_{j=1}^n\ln(w_j^2 + \frac{1}{\mu})
\end{equation}
Здесь отличие от предыдущего метода в том, что тут более совершенный отбор признаков, когда они только совместно обеспечивают хорошее решение.

В общем случае, если функция $f$~--- выпуклая, то можно воспользоваться регуляризацией Moreau-Yosida. Записывается в таком виде для $\lambda > 0$
\begin{equation}
M_{\lambda f}(x) = \underset{u}\inf(f(u) + \frac{1}{2\lambda}||x - u||_2^2) 
\end{equation}
У такой функции ряд замечательных свойств:
\begin{enumerate}
    \item[1)] $M_{\lambda f}(x)$~--- выпуклая функция в силу инфимальной конволюции,\\
    \item[2)] Множество точек минимума для $f$ и $M_{\lambda f}(x)$ совпадают, \\
    \item[3)] $M_{\lambda f}(x)$~--- гладкая функция в силу сильной выпуклости ее первой сопряженной функции и совпадении со второй сопряженной функцие $M_{\lambda f}(x) = M_{\lambda f}^{**}(x)$. \\
\end{enumerate}


\section{Постановка задачи}
\paragraph{Мотивация}
Для решения задачи выбора модели предлагается построить критерий качества.
Сети глубокого обучения могут содержать большое количество параметров, таких как веса нейронов. Это позволяет достичь большей точности предсказания, но приводит к увеличению сложности модели, которую мы понимаем как количество и абсолютное значение ее параметров. Увеличение сложности модели приводит к ее нестабильности, то есть сильной зависимости от изменения начальных данных, а так же к увеличению времени работы. В качестве метода борьбы с увеличением сложности при сохранении точности предлагается использовать аддитивную регуляризацию. Требуется исследовать влияние аддитивной регуляризации на точность и сложность модели глубокого обучения. 

\paragraph{Наборы данных.}
Качество предлагаемого подхода к построению функции ошибки оценивается на нескольких реальных наборах данных и одном синтетическом наборе. Выборки взяты из открытого репозитория данных для машинного обучения ~\cite{seventh}.  Описание всех выборок представлено в табл 1. Синтетический набор данных состоит из признаков с различными свойствами ортогональности и коррелированности друг с другом и к целевой переменной. Процедура генерации синтетических данных описана в работе~\cite{katrutsa2015stress}. Возможны следующие конфигурации синтетических данных.
 \begin{enumerate}
 \item  Неполный и скоррелированный: набор данных, содержащий коррелирующие признаки, ортогональные с целевому вектору.
 \item  Адекватный и случайный: набор данных, содержащий случайные признаки, и имеющий один признак, апроксимирующий целевой вектор.
 \item Адекватный и избыточный: набор данных, содержащий признаки, коррелирующие с целевым вектором.
 \item Адекватный и скорреллированный: набор данных, содержащий ортогональные признаки, и признаки, коррелирующие с ортогональными. Целевой вектор является суммой ортогональных векторов.
 \end{enumerate}
Каждый набор данных разбивается на три части.
\begin{enumerate}
 \item  Обучающая выборка~---~ $60\%$ от исходного набора. На этой выборке модель тренируется, и фиксируются значения параметров. 
 \item  Валидационная выборка~---~ $20\%$ от исходного набора. На этой выборке применяется генетический алгоритм, который ищет оптимальную структуру.
 \item Тестовая выборка~---~ $20\%$ от исходного набора. Она никак не участвует в оптимизации структуры модели. Эта выборка используется только для контроля качества~---~ сравнение модели исходной и оптимизированной структуры, а так же сравнение с другими алгоритмами прореживания сетей. 
 \end{enumerate}

\begin{table}[!htbp]
\captionsetup{justification=raggedright,singlelinecheck=false}
\label{table1}
\caption{Описание выборок для экспериментов}
\footnotesize
\begin{center}
\centering
\begin{tabular}{ | c | c | c |c | c | c | c | }
\hline
Выборка $\mathfrak{D}$ & Размер train  & Размер val  & Размер test& Объекты & Признаки\\
\hline
Credit Card & 18000 & 6000& 6000 & 30000 & 35  \\
\hline
Protein & 27438 & 9146 & 9146 & 45730 & 9 \\
\hline
Airbnb & 6298 & 2100 & 2100 & 10498 & 16 \\
\hline
Wine quality & 2938 & 980 & 980 & 4898 & 11 \\
\hline
Synthetic & 1200 & 400 & 400 & 2000 & 30 \\
\hline
\end{tabular}
\end{center}
\end{table}

\paragraph{Выбор модели}
\\
Задана выборка 
\begin{equation}\label{eq3}
(\xb_i,y_i),\quad \xb_i \in \mathbb{R}^n,\quad y_i\in \mathbb{R}^1,\quad i=1,\dots,m,
\end{equation}
где $\xb$~--- описание объекта, вектор из $n$ элементов признаков, $y$~--- зависимая переменная. Моделью называется отображение $f:(\xb,\wm)\mapsto y$. Требуется построить аппроксимирующую модель $f(\x)$ вида:
\begin{equation}\label{eq:model}
f = \sigma_k\circ\underset{1\times1}{\wm_k^\mathsf{T}\sigmab_{k-1}}\circ\w_{k-1}\sigmab_{k-2}\circ\dots\circ\underset{n_2 \times 1}{\w_2\sigmab_1}\circ\underset{n_1 \times n}{\w_1}\underset{n \times 1}{\x}.
\end{equation}
Эта модель рассматривается как суперпозиция линейной модели, глубокой нейросети и автоэкнодера. Рассмотрим различные модели как частные случаи \eqref{eq:model}.
\\
Линейная или логистическая регрессия и один нейрон~--- имеют вид
\begin{equation}\label{eq11}
f(\xb,\wm)=\sigmab(\wm^\mathsf{T}\xb),
\end{equation}
где $\sigmab$~---~ функция активации, непрерывная монотонная дифференцируемая функция~(\ref{eq49}),  $\wm$~---~ вектор параметров, $\xb$~---~объект, вектор с присоединенным элементом единица соответствующим аддитивному параметру $w_0$. При использовании линейной функции активации,  получаем линейную регрессию $f(\xb,\wm)=\wm^\mathsf{T}\xb.$

Такую функцию активации мы обозначим $\sigmab = \textbf{id} $. При использовании сигмоидной функции активации,  получаем модель логистической регрессии \begin{equation}\label{eq49}
f(\xb,\wm)=\sigma(\wm^\mathsf{T}\xb)=\frac{1}{1+\exp(-\wm^\mathsf{T}\xb)} .
\end{equation}

Двухслойная нейронная сеть, состоящая из линейной комбинации нейронов, однослойных нейронных сетей
\begin{equation}\label{eq12}
f(\xb,\wm)=\sigma^{(2)}\bigg(\sum_{i=1}^{n_2}w_i^{(2)}\cdot\sigma^{(1)}\Bigg(\sum_{j=1}^{n}w_{ij}^{(1)}x_j+w_{i0}^{(1)}\Bigg)+w_0^{(2)}\bigg) = \sigma\circ \wm^{\mathsf{T}}\sigmab\circ \w\xb.
\end{equation}

Метод главных компонент. Модель допускает вращения признакового пространства, то есть координаты (признаки) преобразовываются только с помощью поворотов:
\begin{equation}
\mathbf{h} = \w\x,
\end{equation}
где $\w$~---~ матрица поворота. Она ортогональна: 
\begin{equation}\label{eq51}
\w\w^\mathsf{T} = \mathbf{I}_n.
\end{equation}
Полученное пространство образов $\mathbf{h}$ называется скрытым. Происходит преобразование без потерь.

При удалении нескольких строк матрицы $\w$, например их число $u < n$,  полученный вектор $\mathbf{h}$ имеет размер $u \times 1$. Получается проекция $\mathbf{h}$ вектора $\mathbf{x}$. Согласно теореме Рао~С.Р.~\cite{fourth}, первые $u$ главных компонент восстанавливают $\mathbf{h}$ оптимальным способом,
\begin{equation}\label{eq45}
\textbf{r}(\textbf{x}) = \w^{\mathsf{T}}\mathbf{h}.
\end{equation}

Автокодировщик $\mathbf{h}$~---~ это монотонное нелинейное отображение входного вектора свободных переменных $\textbf{x} \in \mathbb{R}^n$ в скрытое представление $\mathbf{h} \in \mathbb{R}^{u}$ вида:
\begin{equation}\label{eq52}
\mathbf{h}(\x) = \sigmab(\underset{u \times n}{\w}\x + \mathbf{b}) .
\end{equation}
В случае $\sigmab = \textbf{id}$ и \eqref{eq51} автокодировщик тождественен методу главных компонент. Скрытое представление $\textbf{h}$ реконструирует вектор $\textbf{x}$ линейно:
\begin{equation}\label{eq53}
\mathbf{r}(\x) = \underset{n \times u}{\w{'}}\mathbf{h} + \wm_{0}^{'} .
\end{equation}



\paragraph{Функция ошибки и критерии качества модели}

Ключевой идеей данной работы является построение новой функции ошибки с использованием метапараметров аддитивной регуляризации. 

Предлагается использовать композитную функцию ошибки. Она состоит из нескольких слагаемых. Первое слагаемое соответствует точности восстановления зависимой переменной. Второе слагаемое это точность реконструкции независимой переменной автокодировщиком. Остальные $r$ слагаемых отвечают за аддитивную регуляризацию. Задача \eqref{eq:criterion_argmin} является задачей минимизации функции $L$, включающее слагаемое \eqref{eq:mce} и \eqref{eq:autoencoder_error} для оптимизации параметров модели \eqref{eq:model}

\begin{equation}\label{eq:error_function}
L = \lambda_x*E_x + \lambda_y*E_y + \lambda_1S_1+\dots+\lambda_kS_k = E_x + \Smi + \sum\limits_{i = 1}^r\mathbb{\lambda}_i^T\mathbb{S}_i(\mathbf{W}_i).
\end{equation}
Требуется создать каталог слагаемых функции ошибки.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|}
\hline
  Тип регуляризатора   & Роль  \\
  \hline
  $||\mathbf{y} - f(\w)||^2_2 $,   & Ошибка выхода нейронной сети \\
  \hline
  $||\xb-\mathbf{r}(\xb)||^2_2$ & Ошибка восстановления на каждом слое \\
  \hline
  $||\wm-\wm_0||_1$, $||\wm-\wm_0||^2_2$, & $L_1$ и $L_2$ регуляризация\\
  \hline
  $||\w-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от тождественного преобразования \\
  \hline
  $||\w\w^T-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от метода главных компонент \\
  \hline
  $\|\mathbf{T}W\|$ & Тихоновская регуляризация\\
  \hline
\end{tabular}
\caption{Каталог слагаемых функции ошибки}
\end{center}
\end{table}

%\begin{enumerate}
%\item[1)]
%$||\mathbf{y} - f(\w)||^2_2 = E_y$,
%\item[2)]
%$||\xb-\mathbf{r}(\xb)||^2_2$,
%\item[3)]
%$||\wm-\wm_0||_1$,
%$||\wm-\wm_0||^2_2$,
%\item[4)]
%$||\w-\mathbf{I}||$,
%\item[5)]
%$||\w\w^T-\mathbf{I}||$,
%\end{enumerate}


\begin{comment}

Ошибка $E_k$ для каждого слоя будет разная. Пусть $\w$ - матрица параметров одного слоя нейросети, тогда одной строке из этой матрицы будет соответствовать один нейрон. Каждый слой нейросети рассматривается либо в качестве скрытого слоя нейросети, либо в качестве слоя автоенкодера. Тогда $E_x$ – ошибка восстановления на каждом слое. Для каждого слоя с номером $k$ задается ошибка этого слоя
\begin{equation}\label{eq4}
E_k = \lambda_xE_x(\xb-\mathbf{r})+\lambda_yE_y(y-f(\w)) +\lambda_wE(\w)_\text{Frobenius}. 
\end{equation}
Норма фробениуса представляет попарное расстояние от нейрона до нейрона. Требуется увеличить ее значение, чтобы нейроны на одном слое нейросети существенно отличались. Задается одна функция ошибки, с помощью которой будет оптимизироваться структура модели.
Функция ошибки включает штрафы на нейрон, штраф на то, что матрица одного слоя отличается от чистого метода главных компонент. Также требуется исследовать условия перехода от метода главных компонент к методу независимых компонент.

Функция ошибки содержит произведение $k$ элементов на количество ошибок, описанных в таблице, другими словами, на введенное число штрафов. Каждый слой нейросети содержит ошибку и маску $\Gamma$.
Рассморим один слой. Его качественная оценка состоит из слагаемых $||\w_2||, ||\w_1||, E_x, E_y$. А также $||\w||_{\text{Frobenius}}$, которая указывает на разнообразие нейронов в слое. Получается 5*4 слагаемых для 4 слоев нейросети. 
Часть слагаемых может быть вычислена и оптимизирована без основного цикла оптимизации. Автоенкодер требует оптимизации только при наличии 2 слоев и более, потому что если только один – то это метод главных компонент. 
\end{comment}
\begin{comment}



В прошлой работе маска $\Gamma$ оптимизировалась генетическим алгоритмом. В текущей работе необхожимо сделать $\Gamma$ функцией от гиперпараметров, а конкретно от результатов анализа функции ошибки.
При оптимизации используется метод бустрепа и вместо каждого $\w$ оптимального имеется набор оптимальных $\w$, с помощью которых вычисляется ковариационная матрица $A$. Так как дисперсии складываются -  оцениваются var нейрона и var слоя. В прошлой работе в оптимизации не использавалась дисперсия функции ошибки.
Предлагается декомпозировать дисперсию и использовать ее для выбора структуры $\Gamma$. 

\end{comment}

\begin{comment}

???Нейроны в слоях нейросети взаимозаменяемы и
от мультистарта до мультистарта они меняются местами. Желательно решить эту проблему в данной работе. 

\end{comment}

Требуется создать расписание оптимизации параметров регуляризации $\lambda$. Требуется назначать лямбду в зависимости от номера итерации. Если оптимизация сети только началась, то важно подготовить выборку, чтобы на последнем слое нейросети она была простой. Поэтому требуется, чтобы начальные автоекнодеры работали хорошо. Для выбора $\lambda$ предполагается использовать эмпирический подход и исследовать различные техники, как например подходы для выбора шага обучения. Лямбда – точка парето оптимального фронта.

В работе --(прошлая статья)-- поиск структуры работал через добавление или удаление элементов из структуры. Это решение по сути является лихорадочным метанием, которое работает как полный перебор. Требуется предложить стратегию направленного поиска. Такая стратегия, конечно, работает чуть хуже чем полный перебор. 
Алгоритм градиентного спуска сходится довольно быстро. Требуется исследовать возможность поиска оптимальной структуры через сложность. 




В работе \cite{bib_1} представлена таблица свойств различных регуляризаций. В работе рассматриваются нормы со степенью один и два, то есть метрики $L1$ и $L2$. Требуется исследовать свойства метрика с различными значениями степени. 

Используется три вида критериев качества: точность, устойчивость и сложность.

\paragraph{Точность.}

Когда в качестве используемой модели выступает нейросеть или линейная регрессия, 
то функция ошибки имеет вид:
\begin{equation}\label{eq:mce}
\Smi= \sum_{i\in\mathcal{I}}\big(y_i-f(\xb_i)\big)^2.
\end{equation}
Эта функция ошибки включает в себя полученные предсказания модели и значения зависимых переменных. В задачах регрессии точность аппроксимации имеет вид: 
\begin{equation}\label{eq106}
\text{MAE} =\frac{ \sum\limits_{i=1}^m|y_i-f(\xb_i)|}{m}.
\end{equation}

При включении в модель \eqref{eq:model} метода главных компонент или автокодировщика, метки объектов не используются. Функция ошибки штрафует невязки восстановленного объекта:
\begin{equation}\label{eq:autoencoder_error}
E_\mathbf{x} = \sum_{i\in\mathcal{I}}\left\Vert{\xb_i-\mathbf{r}(\xb_i)}\right\Vert_2^2,
\end{equation}
где $\mathbf{r}(\x)$  это линейная реконструкция объекта $\x$. 
Параметры автокодировщика
\begin{equation}
\textbf{W}_{\text{AE}} = \{\w^{'},\w,\mathbf{b}^{'},\mathbf{b}\}
\end{equation}
оптимизированы таким образом (\ref{eq:autoencoder_error}), чтобы приблизить реконструкцию $\mathbf{r}(\x)$ к исходному вектору $\x$.

Процедура оптимизации параметров композитной функции \eqref{eq:error_function}:

\begin{enumerate}
\item[1)]
оптимизируются параметры автокодировщика согласно \eqref{eq:autoencoder_error},
\item[2)]
заданные параметры фиксируются,
\item[3)]
настраиваются метапараметры $\lambda$.
\item[4)]
оптимизируются параметры модели согласно \eqref{eq:mce}.
\end{enumerate}

\paragraph{Сложность.}
Введем отношение порядка $\succ$ на множестве значений сложности. Это отношение задается множеством параметров модели:
\begin{enumerate}
\item[1)] один параметр: $w\in \mathbb{R}^1 \succ w \in \lambda_1 [0,1] +\lambda_0 \succ w\in c +\lambda_0$,
\item[2)] вектор(нейрон): $\wm\in \mathbb{R}^n \succ \left\Vert \wm \right\Vert^2 =1 \succ \wm= \text{const}$,
\item[3)] матрица(слой): $\wvec\in \mathbb{R}^{c{\times}n} \succ \wvec^\mathsf{T}\wvec = \mathbf{I} \succ \wvec= \text{const}$.
\end{enumerate}

Множество, которому принадлежит сложность модели – порядковое. Исходя из введенного понятия сложности модели упорядочены 
по возрастанию сложности:
\begin{enumerate}
\item[1)] линейная регрессия,  $\sigma^{'} = \text{id}, \sigma = \text{id}, \w = \mathbf{I}_n $ ,
\item[2)] линейная регрессия и метод главных компонент, $\sigma^{'} = \text{id}, \w^\mathsf{T}\w = \mathbf{I}_n $ ,
\item[3)] линейная модель и автокодировщик, $\w^\mathsf{T}\w \neq \textbf{I}_n$,
\item[4)] линейная модель и стэк автокодировщиков, представимый в виде суперпозиции~\eqref{eq:model},
\item[5)] двухслойная нейронная сеть,
\item[6)] глубокая нейронная сеть.
\end{enumerate}

\paragraph{Аддитивная регуляризация}
Рассматриваются регуляризации следующих типов:
\begin{enumerate}[label*=\arabic*.]
    \item
    Lasso или $L_1$ регуляризация вида:
    \[S_1(w) = \lambda_1\|w\|_1\]
    \item
    Штраф за количество слоев в нейронной сети:
    \[S_2(k) = \lambda_2 \cdot k\]
    \item
    Штраф за неортогональность матрицы:
    \[S_3(W) = \lambda_3\|WW^T - I\|\]
    \item 
    Несколько видов Тихоновской регуляризации:
    \begin{enumerate}[label*=\arabic*.]
        \item
        Ridge или $L_2$ регуляризация вида:
        \[S_4(w) = \lambda_4\|w\|_2^2\]
        \item
        Штраф за частоту появления весов
        \[
        A = \frac{1}{3} \begin{bmatrix}
            \frac{2}{3}& \frac{2}{3} & 0 & 0 &0 &0& 0\\
            1 & 1 & 1 & 1& 0 & 0 & 0 \\
            0& 0& 1 & 1& 1 & 0 & 0 \\
            0& 0& 0 & 1& 1 & 1 & 0 \\
            0& 0& 0 & 0& 1 & 1 & 1\\
            0& 0& 0 & 0 & 0& \frac{2}{3}& \frac{2}{3}
        \end{bmatrix}
        \]
        \\
        
        \[S_5(W) = \lambda_5 \|(I -A)W\|\]
        \item 
        Штраф за локальную разницу в весах
        \[
        B = \begin{bmatrix}
            -2& 2 & 0 & 0 & 0 &0 &0\\
            -1 & 0 & 1 & 0& 0 & 0 & 0 \\
            0& -1& 0 & 1& 0 & 0 & 0 \\
            0& 0& -1 & 0& 1 & 0 & 0 \\
            0& 0& 0 & -1& 0 & 1 & 0\\
            0& 0& 0 & 0 & 0& -2& 2
        \end{bmatrix}
        \]
        \\
        \[S_6(W) = \lambda_6 \|BW\|\]
    \end{enumerate}
\end{enumerate}

\paragraph{Устойчивость~---}это минимум дисперсии функции ошибки \eqref{eq3}:
\begin{equation}\label{eq103}
\mathsf{D}(S) \rightarrow \min.
\end{equation}
\paragraph{Формулировка задачи}
Таким образом, задача сводится к следующему виду:
\begin{equation}\label{eq:criterion_function}
L(\wm,\mathbb{\lambda}) = \|y - f(\mathbf{w}, x)\|_2^2 + E_x +
\sum\limits_{i = 1}^r\mathbb{\lambda}_i^T\mathbb{S}_i(\mathbf{W}_i)
\end{equation}
\begin{equation}\label{eq:criterion_argmin}
\wm= \arg \min L(f| \mathbb{\lambda})\\
\lambda = \argmin L(f| \wm)
\end{equation}








\section{Расписание оптимизации}
Требуется задать раписание изменений параметра регуляризации $\mathbf{\lambda}$. Предлагается использовать экспертные методы определения расписания изменений $\lambda$. Например, задание $\lambda$ как функцию от номера итерации $t$ обучения нейросети $\lambda(t)=\frac{1}{t}$.

Рассматриваются два варианта использования этого параметра. Первый, обозначим его за $\lambda_S$ - это параметр регуляризации перед ошибкой каждого слоя нейросети \eqref{eq11}. С помощью него регуляризуются слои на разных стадиях обучения. Второй вариант $\lambda_{S_k}$ регулирует различные ошибки в функции ошибки одного слоя \eqref{eq4}. Использование этого параметра регуляризации позволяет нам изменять функцию слоя в структуре всей нейросети. Например, в функции \eqref{eq4} если занулить ошибки $E_y$ и $E_w$, оставив только ошибку $E_x$, то слой будет вести как автокодоривощик. Или наоборот, при слабой регуляризации параметра $E_y$ мы получим нейросеть.
Вообще говоря, $\lambda_{S_k}$ является матрицей $\lambda_{i,q}$, где $i$ соответствует номеру слоя, а $q$ - номеру слагаемого из функции ошибки определенного слоя
\[
\begin{bmatrix}
\lambda_{1,1}& \lambda_{1,2}&\dots&\lambda_{1,q}\\
\dots& \dots&\dots&\dots \\
\lambda_{i,1}& \lambda_{i,2}&\dots&\lambda_{i,q}
\end{bmatrix}
\]
В свою очередь $\lambda_k$ представляет собой вектор
\begin{equation}\label{eq4}
\mathbf{\lambda_k} = \left[\lambda_1,\lambda_2,\dots,\lambda_k \right]
\end{equation}
каждый элемент которого соответствует регуляризационному параметру отдельного слоя.
\section{Вычислительный эксперимент}
Исследуется процедура оптимизации структуры нейросети с сохранением качества аппроксимации. Структура оптимизируется с помощью регуляризации. Цель вычислительного эксперимента состоит в определении оптимальных значений гиперпараметров регуляризации, а так же исследовании зависимости точности,сложности и устойчивости модели от включенных типов регуляризации. Процедура построения модели включает в себя \mbox{следующие} шаги:
\begin{enumerate}
\item
Задание начальной суперпозиции. Для оптимизации параметров используется метод стохастического градиентного спуска.
\item
Оптимизируются гиперпарметры регуляризации.
\item
Для сравнения сложности структуры и исследования зависимости ошибки от сложности вводится отношение порядка на $\mathbf{\Gamma}$.
\item
Строятся графики зависимости ошибки от сложности модели.
\end{enumerate}


Строятся графики зависимости ошибки от числа включенных оптимизаций. Метарпараметры $\lambda$ свои для каждого слоя.
В сети используются активационные функции $ReLU$. Матрица гиперпараметров выглядит следующим образом
\[\{\mathbf{\lambda_1}, \mathbf{\lambda_l}, \mathbf{\lambda_o}, \mathbf{\lambda_2}, \mathbf{\lambda_{hf}}, \mathbf{\lambda_{ld}}\}\],
где:
\begin{itemize}
    \item 
    $\mathbf{\lambda_1}$ -- вектор весов регуляризации для Lasso.
    \item
    $\mathbf{\lambda_l}$ -- вектор весов регуляризации для количества слоев.
    \item
    $\mathbf{\lambda_o}$ -- вектор весов регуляризации ортогональности матрицы весов.
    \item
    $ \mathbf{\lambda_2}$ -- вектор весов регуляризации для Ridge.
    \item
    $\mathbf{\lambda_{hf}}$  -- вектор весов регуляризации частоты весов.
    \item
     $ \mathbf{\lambda_{ld}}$ -- вектор весов регуляризации локальной разницы в матрице весов.
\end{itemize}
Все эти вектора имеют размерность количества слоев.
\section{Заключение}











\begin{comment}


\section{Функция ошибки и критерии качества модели}

Ключевой идеей данной работы является построение новой функции ошибки. 

Предлагается использовать композитную функцию ошибки. Она состоит из нескольких слагаемых. Первое слагаемое соответствует точности восстановления зависимой переменной. Второе слагаемое это точность реконструкции независимой переменной автокодировщиком. Задача \eqref{eq102} является задачей минимизации функции $S$, включающее слагаемое \eqref{eq3} и \eqref{eq54} для оптимизации параметров модели \eqref{eq58}

\begin{equation}
S(\wm|\mathbf{\gamma},D) = S(\wm),
\end{equation}
\begin{equation}\label{eq11}
S = \lambda_1S_1+\dots+\lambda_kS_k = \mathbb{\lambda}^{\mathsf{T}}\mathbb{S}.
\end{equation}
Требуется создать каталог слагаемых функции ошибки.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|}
\hline
  Тип регуляризатора/слагаемого   & Роль  \\
  \hline
  $||\mathbf{y} - f(\w)||^2_2 = E_y$,   & Ошибка выхода нейронной сети \\
  \hline
  $||\xb-\mathbf{r}(\xb)||^2_2$ & Ошибка восстановления на каждом слое \\
  \hline
  $||\wm-\wm_0||_1$, $||\wm-\wm_0||^2_2$, & \\
  \hline
  $||\w-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от тождественного преобразования \\
  \hline
  $||\w\w^T-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от метода главных компонент \\
  \hline
\end{tabular}
\caption{Каталог слагаемых функции ошибки}
\end{center}
\end{table}

%\begin{enumerate}
%\item[1)]
%$||\mathbf{y} - f(\w)||^2_2 = E_y$,
%\item[2)]
%$||\xb-\mathbf{r}(\xb)||^2_2$,
%\item[3)]
%$||\wm-\wm_0||_1$,
%$||\wm-\wm_0||^2_2$,
%\item[4)]
%$||\w-\mathbf{I}||$,
%\item[5)]
%$||\w\w^T-\mathbf{I}||$,
%\end{enumerate}

Ошибка $E_k$ для каждого слоя будет разная. Пусть $\w$ - матрица параметров одного слоя нейросети, тогда одной строке из этой матрицы будет соответствовать один нейрон. Каждый слой нейросети рассматривается либо в качестве скрытого слоя нейросети, либо в качестве слоя автоенкодера. Тогда $E_x$ – ошибка восстановления на каждом слое. Для каждого слоя с номером $k$ задается ошибка этого слоя
\begin{equation}\label{eq4}
E_k = \lambda_xE_x(\xb-\mathbf{r})+\lambda_yE_y(y-f(\w)) +\lambda_wE(\w)_\text{Frobenius}. 
\end{equation}
Норма фробениуса представляет попарное расстояние от нейрона до нейрона. Требуется увеличить ее значение, чтобы нейроны на одном слое нейросети существенно отличались. Задается одна функция ошибки, с помощью которой будет оптимизироваться структура модели.
Функция ошибки включает штрафы на нейрон, штраф на то, что матрица одного слоя отличается от чистого метода главных компонент. Также требуется исследовать условия перехода от метода главных компонент к методу независимых компонент.

Функция ошибки содержит произведение $k$ элементов на количество ошибок, описанных в таблице, другими словами, на введенное число штрафов. Каждый слой нейросети содержит ошибку и маску $\Gamma$.
Рассморим один слой. Его качественная оценка состоит из слагаемых $||\w_2||, ||\w_1||, E_x, E_y$. А также $||\w||_{\text{Frobenius}}$, которая указывает на разнообразие нейронов в слое. Получается 5*4 слагаемых для 4 слоев нейросети. 
Часть слагаемых может быть вычислена и оптимизирована без основного цикла оптимизации. Автоенкодер требует оптимизации только при наличии 2 слоев и более, потому что если только один – то это метод главных компонент. 

В прошлой работе маска $\Gamma$ оптимизировалась генетическим алгоритмом. В текущей работе необхожимо сделать $\Gamma$ функцией от гиперпараметров, а конкретно от результатов анализа функции ошибки.
При оптимизации используется метод бустрепа и вместо каждого $\w$ оптимального имеется набор оптимальных $\w$, с помощью которых вычисляется ковариационная матрица $A$. Так как дисперсии складываются -  оцениваются var нейрона и var слоя. В прошлой работе в оптимизации не использавалась дисперсия функции ошибки.
Предлагается декомпозировать дисперсию и использовать ее для выбора структуры $\Gamma$. 


???Нейроны в слоях нейросети взаимозаменяемы и
от мультистарта до мультистарта они меняются местами. Желательно решить эту проблему в данной работе. 

Требуется создать расписание оптимизации параметров регуляризации $\lambda$. Требуется назначать метапараметр в зависимости от номера итерации. Если оптимизация сети только началась, то нам важно подготовить выборку, чтобы на последнем слое нейросети она была простой. Поэтому требуется, чтобы начальные автоенкодеры восстанавливали исходный объект хорошо. Для выбора $\lambda$ предполагается использовать эмпирический подход и исследовать различные техники, как например подходы для выбора шага обучения. Лямбда – точка парето оптимального фронта.

В прошлой работе поиск структуры работал через добавление или удаление элементов из структуры. Это решение по сути является лихорадочным метанием, которое работает как полный перебор. Требуется предложить стратегию направленного поиска. Такая стратегия, конечно, работает чуть хуже чем полный перебор. 
Алгоритм градиентного спуска сходится довольно быстро. Требуется исследовать возможность поиска оптимальной структуры через сложность. 




В работе \cite{bib_1} представлена таблица свойств различных регуляризаций. В работе рассматриваются нормы со степенью один и два, то есть метрики $L1$ и $L2$. Требуется исследовать свойства метрика с различными значениями степени. 
Первое слагаемое $E_{\textbf{x}}$~--- это функция ошибки реконструкции объекта стеком автокодировщиков. Второе слагаемое $S$~--- это функция ошибки нейросети.

При выборе моделей используется три вида критериев качества: точность, устойчивость и сложность.

\paragraph{Точность.}

Когда в качестве используемой модели выступает нейросеть или линейная регрессия, 
то функция ошибки имеет вид:
\begin{equation}\label{eq3}
S = \sum_{i\in\mathcal{I}}\big(y_i-f(\xb_i)\big)^2.
\end{equation}
Эта функция ошибки включает в себя полученные предсказания модели и значения зависимых переменных. В задачах регрессии точность аппроксимации имеет вид: 
\begin{equation}\label{eq106}
\text{MAE} =\frac{ \sum\limits_{i=1}^m|y_i-f(\xb_i)|}{m}.
\end{equation}

При включении в модель \eqref{eq44} метода главных компонент или автокодировщика, метки объектов не используются. Функция ошибки штрафует невязки восстановленного объекта:
\begin{equation}\label{eq4}
E_\mathbf{x} = \sum_{i\in\mathcal{I}}\left\Vert{\xb_i-\mathbf{r}(\xb_i)}\right\Vert_2^2,
\end{equation}
где $\mathbf{r}(\x)$  это линейная реконструкция объекта $\x$. Функция \eqref{eq4} с аддитивной регуляризацией:
\begin{equation}\label{eq54}
E_\mathbf{x} = \frac{1}{2m}\sum\limits_{i=1}^m||\mathbf{r}(\textbf{x}_i,\textbf{W}_{\text{AE}}) - \x_i||^2 + \lambda^2||\w||_\text{Frobenius}^2,
\end{equation}
где $m$~---~ число элементов в обучающей выборке.
Параметры автокодировщика
\begin{equation}
\textbf{W}_{\text{AE}} = \{\w^{'},\w,\mathbf{b}^{'},\mathbf{b}\}
\end{equation}
оптимизированы таким образом (\ref{eq4}), чтобы приблизить реконструкцию $\mathbf{r}(\x)$ к исходному вектору $\x$.

Процедура оптимизации параметров композитной функции \eqref{eq58}:

\begin{enumerate}
\item[1)]
оптимизируются параметры модели согласно \eqref{eq54},
\item[2)]
заданные параметры фиксируются,
\item[3)]
оптимизируются параметры согласно \eqref{eq3}.
\end{enumerate}

\paragraph{Сложность.}
Введем отношение порядка $\succ$ на множестве значений сложности. Это отношение задается множеством параметров модели:
\begin{enumerate}
\item[1)] один параметр: $w\in \mathbb{R}^1 \succ w \in \lambda_1 [0,1] +\lambda_0 \succ w\in c +\lambda_0$,
\item[2)] вектор(нейрон): $\wm\in \mathbb{R}^n \succ \left\Vert \wm \right\Vert^2 =1 \succ \wm= \text{const}$,
\item[3)] матрица(слой): $\wvec\in \mathbb{R}^{c{\times}n} \succ \wvec^\mathsf{T}\wvec = \mathbf{I} \succ \wvec= \text{const}$.
\end{enumerate}

Множество, которому принадлежит сложность модели – порядковое. Исходя из введенного понятия сложности модели упорядочены 
по возрастанию сложности:
\begin{enumerate}
\item[1)] линейная регрессия,  $\sigma^{'} = \text{id}, \sigma = \text{id}, \w = \mathbf{I}_n $ ,
\item[2)] линейная регрессия и метод главных компонент, $\sigma^{'} = \text{id}, \w^\mathsf{T}\w = \mathbf{I}_n $ ,
\item[3)] линейная модель и автокодировщик, $\w^\mathsf{T}\w \neq \textbf{I}_n$,
\item[4)] линейная модель и стэк автокодировщиков, представимый в виде суперпозиции~\eqref{eq57},
\item[5)] двухслойная нейронная сеть,
\item[6)] глубокая нейронная сеть.
\end{enumerate}

\paragraph{Устойчивость~---}это минимум дисперсии функции ошибки \eqref{eq3}:
\begin{equation}\label{eq103}
\mathsf{D}(S) \rightarrow \min.
\end{equation}

your model in the class of models,
restrictions on the class of models,
the error function (and its inference) or a loss function, or a quality criterion,
cross-validation procedure,
restrictions to the solutions,
external (industrial) quality criteria,
the optimization statement as argmin.





















\paragraph{Выбор модели}

Задана выборка 
\begin{equation}\label{eq3}
(\xb_i,y_i),\quad \xb_i \in \mathbb{R}^n,\quad y_i\in \mathbb{R}^1,\quad i=1,\dots,m,
\end{equation}
где $\xb$~--- описание объекта, вектор из $n$ элементов признаков, $y$~--- зависимая переменная. Моделью называется отображение $f:(\xb,\wm)\mapsto y$. Требуется построить аппроксимирующую модель $f(\x)$ вида:
\begin{equation}\label{eq44}
f = \sigma_k\circ\underset{1\times1}{\wm_k^\mathsf{T}\sigmab_{k-1}}\circ\w_{k-1}\sigmab_{k-2}\circ\dots\circ\underset{n_2 \times 1}{\w_2\sigmab_1}\circ\underset{n_1 \times n}{\w_1}\underset{n \times 1}{\x}.
\end{equation}
Эта модель рассматривается как суперпозиция линейной модели, глубокой нейросети и автоэкнодера. Рассмотрим различные модели как частные случаи (\ref{eq44}).
\\
Линейная или логистическая регрессия и один нейрон~--- имеют вид
\begin{equation}\label{eq11}
f(\xb,\wm)=\sigmab(\wm^\mathsf{T}\xb),
\end{equation}
где $\sigmab$~---~ функция активации, непрерывная монотонная дифференцируемая функция~(\ref{eq49}),  $\wm$~---~ вектор параметров, $\xb$~---~объект, вектор с присоединенным элементом единица соответствующим аддитивному параметру $w_0$. При использовании линейной функции активации,  получаем линейную регрессию $f(\xb,\wm)=\wm^\mathsf{T}\xb.$

Такую функцию активации мы обозначим $\sigmab = \textbf{id} $. При использовании сигмоидной функции активации,  получаем модель логистической регрессии \begin{equation}\label{eq49}
f(\xb,\wm)=\sigma(\wm^\mathsf{T}\xb)=\frac{1}{1+\exp(-\wm^\mathsf{T}\xb)} .
\end{equation}

Двухслойная нейронная сеть, состоящая из линейной комбинации нейронов, однослойных нейронных сетей
\begin{equation}\label{eq12}
f(\xb,\wm)=\sigma^{(2)}\bigg(\sum_{i=1}^{n_2}w_i^{(2)}\cdot\sigma^{(1)}\Bigg(\sum_{j=1}^{n}w_{ij}^{(1)}x_j+w_{i0}^{(1)}\Bigg)+w_0^{(2)}\bigg) = \sigma\circ \wm^{\mathsf{T}}\sigmab\circ \w\xb.
\end{equation}

Метод главных компонент. Модель допускает вращения признакового пространства, то есть координаты (признаки) преобразовываются только с помощью поворотов:
\begin{equation}
\mathbf{h} = \w\x,
\end{equation}
где $\w$~---~ матрица поворота. Она ортогональна: 
\begin{equation}\label{eq51}
\w\w^\mathsf{T} = \mathbf{I}_n.
\end{equation}
Полученное пространство образов $\mathbf{h}$ называется скрытым. Происходит преобразование без потерь.

При удалении нескольких строк матрицы $\w$, например их число $u < n$,  полученный вектор $\mathbf{h}$ имеет размер $u \times 1$. Получается проекция $\mathbf{h}$ вектора $\mathbf{x}$. Согласно теореме Рао~С.Р.~\cite{fourth}, первые $u$ главных компонент восстанавливают $\mathbf{h}$ оптимальным способом,
\begin{equation}\label{eq45}
\textbf{r}(\textbf{x}) = \w^{\mathsf{T}}\mathbf{h}.
\end{equation}

Автокодировщик $\mathbf{h}$~---~ это монотонное нелинейное отображение входного вектора свободных переменных $\textbf{x} \in \mathbb{R}^n$ в скрытое представление $\mathbf{h} \in \mathbb{R}^{u}$ вида:
\begin{equation}\label{eq52}
\mathbf{h}(\x) = \sigmab(\underset{u \times n}{\w}\x + \mathbf{b}) .
\end{equation}
В случае $\sigmab = \textbf{id}$ и \eqref{eq51} автокодировщик тождественен методу главных компонент. Скрытое представление $\textbf{h}$ реконструирует вектор $\textbf{x}$ линейно:
\begin{equation}\label{eq53}
\mathbf{r}(\x) = \underset{n \times u}{\w{'}}\mathbf{h} + \wm_{0}^{'} .
\end{equation}


\section{Задача выбора оптимальной структуры модели}

Решается задача выбора оптимальной структуры модели
\begin{equation}\label{eq57}
f = \sigma_k\circ\boldsymbol{\Gamma}_k\otimes\underset{1\times1}{\wm_k^\mathsf{T}\sigmab_{k-1}}\circ\boldsymbol{\Gamma}_{k-1}\otimes\w_{k-1}\sigmab_{k-2}\circ\dots\circ\boldsymbol{\Gamma}_2\otimes\underset{n_2 \times 1}{\w_2\sigmab_1}\circ\boldsymbol{\Gamma}_1\otimes\underset{n_1 \times n}{\w_1}\underset{n \times 1}{\x},
\end{equation}
где $\boldsymbol{\Gamma}$~---~ матрица, задающая структуру модели; $\otimes$~--- адамарово произведение, определяющееся как поэлементное умножение. Если элемент $\gamma\in\{0,1\}$ матрицы $\boldsymbol{\Gamma}$ равен нулю, то соответствующий элемент матрицы параметров $\w$ обнуляется, и не участвует в работе модели. Множество индексов соответствующих ненулевым элементам матрицы $\boldsymbol{\Gamma}$ обозначается $\mathcal{A}$.  Требуется найти такое подмножество индексов $\mathcal{A}^{*}$, которое доставляет минимум функции:
\begin{equation}\label{eq46}
\mathcal{A}^{*} = \arg \underset{\mathcal{A} \subseteq \mathcal{I}}\min S(f_{\mathcal{A}}|\wm^*, \mathfrak{D}_\mathcal{C}),
\end{equation}
на разбиении выборки $\mathfrak{D}$, определенным множеством индексов $\mathcal{C}$. Здесь $\mathcal{I} = \mathcal{C}\sqcup \mathcal{L}$~---~ все индексы всех матриц $\boldsymbol{\Gamma}$. То есть требуется снизить число признаков и повысить устойчивость модели.

При этом параметры $\textbf{w}^*$ модели доставляют минимум ошибки:
\begin{equation}\label{eq102}
\textbf{w}^* = \arg \underset{\textbf{w}}\min S(\textbf{w}|\mathfrak{D}_\mathcal{L}, f_\mathcal{A}),
\end{equation}
на разбиении выборки, определенной множеством $\mathcal{L}$. Процедура разбиения описана в вычислительном эксперименте.



--------Здесь копипаст заканчивается ------------





Нейросеть является суперпозицией линейных моделей. В отличие от работы \cite{bib_4}, в данной работе предлагается заменить генетический алгоритм квадратичным программированием или градиентным спуском, построив построив новую функцию ошибки $S$.
\begin{equation}
S(\wm|\mathbf{\gamma},D) = S(\wm),
\end{equation}
\begin{equation}\label{eq11}
S = \lambda_1S_1+\dots+\lambda_kS_k = \mathbb{\lambda}^{\mathsf{T}}\mathbb{S}.
\end{equation}
Требуется создать каталог слагаемых функции ошибки.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|}
\hline
  Тип регуляризатора/слагаемого   & Роль  \\
  \hline
  $||\mathbf{y} - f(\w)||^2_2 = E_y$,   & Ошибка выхода нейронной сети \\
  \hline
  $||\xb-\mathbf{r}(\xb)||^2_2$ & Ошибка восстановления на каждом слое \\
  \hline
  $||\wm-\wm_0||_1$, $||\wm-\wm_0||^2_2$, & \\
  \hline
  $||\w-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от тождественного преобразования \\
  \hline
  $||\w\w^T-\mathbf{I}||$ & Штраф за различие матрицы одного слоя \\
  & от метода главных компонент \\
  \hline
\end{tabular}
\caption{Каталог слагаемых функции ошибки}
\end{center}
\end{table}

%\begin{enumerate}
%\item[1)]
%$||\mathbf{y} - f(\w)||^2_2 = E_y$,
%\item[2)]
%$||\xb-\mathbf{r}(\xb)||^2_2$,
%\item[3)]
%$||\wm-\wm_0||_1$,
%$||\wm-\wm_0||^2_2$,
%\item[4)]
%$||\w-\mathbf{I}||$,
%\item[5)]
%$||\w\w^T-\mathbf{I}||$,
%\end{enumerate}

Ошибка $E_k$ для каждого слоя будет разная. Пусть $\w$ - матрица параметров одного слоя нейросети, тогда одной строке из этой матрицы будет соответствовать один нейрон. Каждый слой нейросети рассматривается либо в качестве скрытого слоя нейросети, либо в качестве слоя автоенкодера. Тогда $E_x$ – ошибка восстановления на каждом слое. Для каждого слоя с номером $k$ задается ошибка этого слоя
\begin{equation}\label{eq4}
E_k = \lambda_xE_x(\xb-\mathbf{r})+\lambda_yE_y(y-f(\w)) +\lambda_wE(\w)_\text{Frobenius}. 
\end{equation}
Норма фробениуса представляет попарное расстояние от нейрона до нейрона. Требуется увеличить ее значение, чтобы нейроны на одном слое нейросети существенно отличались. Задается одна функция ошибки, с помощью которой будет оптимизироваться структура модели.
Функция ошибки включает штрафы на нейрон, штраф на то, что матрица одного слоя отличается от чистого метода главных компонент. Также требуется исследовать условия перехода от метода главных компонент к методу независимых компонент.

Функция ошибки содержит произведение $k$ элементов на количество ошибок, описанных в таблице, другими словами, на введенное число штрафов. Каждый слой нейросети содержит ошибку и маску $\Gamma$.
Рассморим один слой. Его качественная оценка состоит из слагаемых $||\w_2||, ||\w_1||, E_x, E_y$. А также $||\w||_{\text{Frobenius}}$, которая указывает на разнообразие нейронов в слое. Получается 5*4 слагаемых для 4 слоев нейросети. 
Часть слагаемых может быть вычислена и оптимизирована без основного цикла оптимизации. Автоенкодер требует оптимизации только при наличии 2 слоев и более, потому что если только один – то это метод главных компонент. 

В прошлой работе маска $\Gamma$ оптимизировалась генетическим алгоритмом. В текущей работе необхожимо сделать $\Gamma$ функцией от гиперпараметров, а конкретно от результатов анализа функции ошибки.
При оптимизации используется метод бустрепа и вместо каждого $\w$ оптимального имеется набор оптимальных $\w$, с помощью которых вычисляется ковариационная матрица $A$. Так как дисперсии складываются -  оцениваются var нейрона и var слоя. В прошлой работе в оптимизации не использавалась дисперсия функции ошибки.
Предлагается декомпозировать дисперсию и использовать ее для выбора структуры $\Gamma$. 


???Нейроны в слоях нейросети взаимозаменяемы и
от мультистарта до мультистарта они меняются местами. Желательно решить эту проблему в данной работе. 

Требуется создать расписание оптимизации параметров регуляризации $\lambda$. Требуется назначать лямбду в зависимости от номера итерации. Если оптимизация сети только началась, то нам важно подготовить выборку, чтобы на последнем слое нейросети она была простой. Поэтому требуется, чтобы начальные автоекнодеры работали хорошо. Для выбора $\lambda$ предполагается использовать эмпирический подход и исследовать различные техники, как например подходы для выбора шага обучения. Лямбда – точка парето оптимального фронта.

В прошлой работе поиск структуры работал через добавление или удаление элементов из структуры. Это решение по сути является лихорадочным метанием, которое работает как полный перебор. Требуется предложить стратегию направленного поиска. Такая стратегия, конечно, работает чуть хуже чем полный перебор. 
Алгоритм градиентного спуска сходится довольно быстро. Требуется исследовать возможность поиска оптимальной структуры через сложность. 




В работе \cite{wang2018learning} представлена таблица свойств различных регуляризаций. В работе рассматриваются нормы со степенью один и два, то есть метрики $L1$ и $L2$. Требуется исследовать свойства метрика с различными значениями степени. 




\section{Оптимизация структуры}
Иерархия структуры нейросети имеет следующий вид
$$\text{параметр} \prec \text{нейрон} \prec \text{слой} \prec \text{сеть}$$.
Оптимизация нейросети состоит из следующих шагов:
\begin{enumerate}
\item[1)]
Оптимизируем параметры,
\item[2)]
Оптимизируем структуру,
\end{enumerate}

\paragraph{Сложность.}
Введем отношение порядка $\succ$ на множестве значений сложности. Это отношение задается множеством параметров модели:
\begin{enumerate}[1)]
\item один параметр: $w\in \mathbb{R}^1 \succ w \in \lambda_1 [0,1] +\lambda_0 \succ w\in c +\lambda_0$,
\item вектор (нейрон): $\wm\in \mathbb{R}^n \succ \left\Vert \wm \right\Vert^2 =1 \succ \wm= \text{const}$,
\item матрица (слой): $\wvec\in \mathbb{R}^{c{\times}n} \succ \wvec^\mathsf{T}\wvec = \mathbf{I} \succ \wvec= \text{const}$.
\end{enumerate}

Устойчивость модели не возрастает с ее сложностью. Информативность каждого слоя, каждого нейрона, каждого элемента падает. 
Сложность задается как отношение порядка для множестве значений сложности. Структура которая включает 1 параметр проще структуры, которая включает два параметра. А две разные структуры с 1 параметров несравнимы с точки зрения сложности.
Следующие модели упорядочены по возрастанию сложности:
\begin{enumerate}[1)]
\item линейная регрессия,  $\sigma^{'} = \text{id}, \sigma = \text{id}, \w = \mathbf{I}_n $ ,
\item линейная регрессия и метод главных компонент, $\sigma^{'} = \text{id}, \w^\mathsf{T}\w = \mathbf{I}_n $ ,
\item линейная модель и автокодировщик, $\w^\mathsf{T}\w \neq \textbf{I}_n$,
\item линейная модель и стэк автокодировщиков, представимый в виде суперпозиции~\eqref{eq57},
\item двухслойная нейронная сеть,
\item глубокая нейронная сеть.
\end{enumerate}
\section{Расписание оптимизации}
Требуется задать раписание изменений параметра регуляризации $\mathbf{\lambda}$. Предлагается использовать экспертные методы определения расписания изменений $\lambda$. Например, задание $\lambda$ как функцию от номера итерации $t$ обучения нейросети $\lambda(t)=\frac{1}{t}$.

Рассматриваются два варианта использования этого параметра. Первый, обозначим его за $\lambda_S$ - это параметр регуляризации перед ошибкой каждого слоя нейросети \eqref{eq11}. С помощью него регуляризуются слои на разных стадиях обучения. Второй вариант $\lambda_{S_k}$ регулирует различные ошибки в функции ошибки одного слоя \eqref{eq4}. Использование этого параметра регуляризации позволяет нам изменять функцию слоя в структуре всей нейросети. Например, в функции \eqref{eq4} если занулить ошибки $E_y$ и $E_w$, оставив только ошибку $E_x$, то слой будет вести как автокодоривощик. Или наоборот, при слабой регуляризации параметра $E_y$ мы получим нейросеть.
Вообще говоря, $\lambda_{S_k}$ является матрицей $\lambda_{i,q}$, где $i$ соответствует номеру слоя, а $q$ - номеру слагаемого из функции ошибки определенного слоя
\[
\begin{bmatrix}
\lambda_{1,1}& \lambda_{1,2}&\dots&\lambda_{1,q}\\
\dots& \dots&\dots&\dots \\
\lambda_{i,1}& \lambda_{i,2}&\dots&\lambda_{i,q}
\end{bmatrix}
\]
В свою очередь $\lambda_k$ представляет собой вектор
\begin{equation}\label{eq4}
\mathbf{\lambda_k} = \left[\lambda_1,\lambda_2,\dots,\lambda_k \right]
\end{equation}
каждый элемент которого соответствует регуляризационному параметру отдельного слоя.
\section{Вычислительный эксперимент}
Исследуется процедура оптимизации структуры нейросети с сохранением качества аппроксимации. Структура оптимизируется с помощью регуляризации. Цель вычислительного эксперимента состоит в определении оптимальных значений гиперпараметров регуляризации, а так же исследовании зависимости точности,сложности и устойчивости модели от включенных типов регуляризации. Процедура построения модели включает в себя \mbox{следующие} шаги:
\begin{enumerate}
\item
Задание начальной суперпозиции. Для оптимизации параметров используется метод стохастического градиентного спуска.
\item
Оптимизируются гиперпарметры регуляризации.
\item
Для сравнения сложности структуры и исследования зависимости ошибки от сложности вводится отношение порядка на $\mathbf{\Gamma}$.
\item
Строятся графики зависимости ошибки от сложности модели.
\end{enumerate}

Рассматриваются регуляризации следующих типов:
\begin{enumerate}[label*=\arabic*.]
    \item
    Lasso или $L_1$ регуляризация вида:
    \[L_1(w) = \lambda_1\|w\|_1\]
    \item
    Штраф за количество слоев в нейронной сети:
    \[L_l(k) = \lambda_l \cdot k\]
    \item
    Штраф за неортогональность матрицы:
    \[L_o(W) = \lambda_o\|WW^T - I\|\]
    \item 
     \href{https://towardsdatascience.com/tikhonov-regularization-an-example-other-than-l2-8922ba51253d}{Несколько видов Тихоновской регуляризации}:
    \begin{enumerate}[label*=\arabic*.]
        \item
        Ridge или $L_2$ регуляризация вида:
        \[L_1(w) = \lambda_2\|w\|_2^2\]
        \item
        Штраф за частоту появления весов
        \[
        A = \frac{1}{3} \begin{bmatrix}
            \frac{2}{3}& \frac{2}{3} & 0 & 0 &0 &0& 0\\
            1 & 1 & 1 & 1& 0 & 0 & 0 \\
            0& 0& 1 & 1& 1 & 0 & 0 \\
            0& 0& 0 & 1& 1 & 1 & 0 \\
            0& 0& 0 & 0& 1 & 1 & 1\\
            0& 0& 0 & 0 & 0& \frac{2}{3}& \frac{2}{3}
        \end{bmatrix}
        \]
        \\
        
        \[L_{hf}(W) = \lambda_{hf} \|(I -A)W\|\]
        \item 
        Штраф за локальную разницу в весах
        \[
        B = \begin{bmatrix}
            -2& 2 & 0 & 0 & 0 &0 &0\\
            -1 & 0 & 1 & 0& 0 & 0 & 0 \\
            0& -1& 0 & 1& 0 & 0 & 0 \\
            0& 0& -1 & 0& 1 & 0 & 0 \\
            0& 0& 0 & -1& 0 & 1 & 0\\
            0& 0& 0 & 0 & 0& -2& 2
        \end{bmatrix}
        \]
        \\
        \[L_{ld}(W) = \lambda_{ld} \|BW\|\]
    \end{enumerate}
\end{enumerate}
Строятся графики зависимости ошибки от числа включенных оптимизаций. Гиперпараметры $\lambda$ свои для каждого слоя.
В сети используются активационные функции $ReLU$. Матрица гиперпараметров выглядит следующим образом
\[\{\mathbf{\lambda_1}, \mathbf{\lambda_l}, \mathbf{\lambda_o}, \mathbf{\lambda_2}, \mathbf{\lambda_{hf}}, \mathbf{\lambda_{ld}}\}\],
где:
\begin{itemize}
    \item 
    $\mathbf{\lambda_1}$ -- вектор весов регуляризации для Lasso.
    \item
    $\mathbf{\lambda_l}$ -- вектор весов регуляризации для количества слоев.
    \item
    $\mathbf{\lambda_o}$ -- вектор весов регуляризации ортогональности матрицы весов.
    \item
    $ \mathbf{\lambda_2}$ -- вектор весов регуляризации для Ridge.
    \item
    $\mathbf{\lambda_{hf}}$  -- вектор весов регуляризации частоты весов.
    \item
     $ \mathbf{\lambda_{ld}}$ -- вектор весов регуляризации локальной разницы в матрице весов.
\end{itemize}
Все эти вектора имеют размерность количества слоев.
\section{Заключение}

\end{comment}

\bibliographystyle{unsrt}
\bibliography{References}

\begin{comment}

\begin{thebibliography}{4}
\bibitem{bib_1} Wang J. et al. Learning credible models //Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. – ACM, 2018. – С. 2417-2426.
\bibitem{bib_2} Tibshirani R. Regression shrinkage and selection via the lasso //Journal of the Royal Statistical Society: Series B (Methodological). – 1996. – Т. 58. – №. 1. – С. 267-288.
\bibitem{bib_3} Zou H., Hastie T. Regularization and variable selection via the elastic net //Journal of the royal statistical society: series B (statistical methodology). – 2005. – Т. 67. – №. 2. – С. 301-320.
\bibitem{bib_4} Ссылка на нашу статью.
\end{thebibliography}
\end{comment}
\end{document}

